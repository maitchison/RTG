"""
PPO implementation for Multi-Agent Reinforcement Learning

Planned Features

[ ] Role Prediction
[ ] Prediction of other players observations
[ ] Prediction of other players prediction of our own observation
[ ] Optional Shared value function during training
[ ] micro batching for low GPU machines (using gradient accumulation)

Notation

b: number of agents calculated simultaneously (effectively the batch size used during rollout)
p: (max) number of players in game
r: number of roles
a: number of actions

batch: the total batch of data generated by a rollout, i.e. 64k
minibatch: the batch size used during optimizer updates, often batch/4, e.g. 16k
microbatch: smaller batches used due to GPU memory limitations, gradients are accumulated during microbatch processing
    making them equivalent to a larger minibatch size, but with less memory (this does not allow for batch-normalizion
    however)

extrinsic and intrinsic rewards are learned separately as value_int, and value_ext

"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import GradScaler, autocast
from logger import Logger
from typing import Union

import utils

from marl_env import MultiAgentVecEnv

class BaseEncoder(nn.Module):

    def __init__(self, input_dims, out_features):
        """
        :param input_dims: intended dims of input, excluding batch size (height, width, channels)
        """
        super().__init__()

        self.input_dims = input_dims
        self.final_dims = None
        self.out_features = out_features

    def forward(self, x):
        raise NotImplemented()

class DefaultEncoder(BaseEncoder):
    """ Encodes from observation to hidden representation. """

    def __init__(self, input_dims, out_features=128):
        """
        :param input_dims: intended dims of input, excluding batch size (height, width, channels)
        """
        super().__init__(input_dims, out_features)

        self.final_dims = (64, self.input_dims[0]//2//2, self.input_dims[1]//2//2)

        self.conv1 = nn.Conv2d(self.input_dims[2], 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.fc = nn.Linear(prod(self.final_dims), self.out_features)

        print(f" -created default encoder, final dims {self.final_dims}")

    def forward(self, x):
        """
        input float32 tensor of dims [b, h, w, c]
        return output tensor of dims [b, d], where d is the number of units in final layer.
        """
        assert x.shape[1:] == self.input_dims, f"Input dims {x.shape[1:]} must match {self.input_dims}"
        assert x.dtype == torch.float32, f"Datatype should be torch.float32 not {x.dtype}"

        b = x.shape[0]

        # put in BCHW format for pytorch.
        x = x.transpose(2, 3)
        x = x.transpose(1, 2)

        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.max_pool2d(x, 2, 2)
        x = torch.relu(self.conv3(x))
        x = torch.max_pool2d(x, 2, 2)
        assert x.shape[1:] == self.final_dims, f"Expected final shape to be {self.final_dims} but found {x.shape[1:]}"
        x = x.reshape((b, -1))
        x = torch.relu(self.fc(x))

        return x

class FastEncoder(BaseEncoder):
    """ Encodes from observation to hidden representation.
        Optimized to be efficient on CPU
    """

    def __init__(self, input_dims, out_features=64):
        """
        :param input_dims: intended dims of input, excluding batch size (height, width, channels)
        """
        super().__init__(input_dims, out_features)

        self.final_dims = (64, self.input_dims[0]//3//2, self.input_dims[1]//3//2)

        self.conv1 = nn.Conv2d(self.input_dims[2], 32, kernel_size=3, stride=3)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.fc = nn.Linear(prod(self.final_dims), self.out_features)

        print(f"Created fast encoder, final dims {self.final_dims}")

    def forward(self, x):
        """
        input float32 tensor of dims [b, h, w, c]
        return output tensor of dims [b, d], where d is the number of units in final layer.
        """

        assert x.shape[1:] == self.input_dims, f"Input dims {x.shape[1:]} must match {self.input_dims}"
        assert x.dtype == torch.float32, f"Datatype should be torch.float32 not {x.dtype}"

        b = x.shape[0]

        # put in BCHW format for pytorch.
        x = x.transpose(2, 3)
        x = x.transpose(1, 2)

        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = torch.max_pool2d(x, 2, 2)
        assert x.shape[1:] == self.final_dims, f"Expected final shape to be {self.final_dims} but found {x.shape[1:]}"
        x = x.reshape((b, -1))

        x = torch.relu(self.fc(x))

        return x

class BasicDecoder(nn.Module):
    """ Decodes from hidden representation to observation. """

    def __init__(self, output_dims, hidden_features=128):

        super().__init__()

        self.output_dims = output_dims # (c, h, w)
        self.initial_dims = (64, self.output_dims[1]//4, self.output_dims[0]//4)
        self.hidden_features = hidden_features

        self.deconv1 = nn.ConvTranspose2d(64, 64, 3)
        self.deconv2 = nn.ConvTranspose2d(64, 32, 3)
        self.deconv3 = nn.ConvTranspose2d(32, self.output_dims[0], 3)

        self.fc = nn.Linear(self.hidden_features, prod(self.initial_dims))

    def forward(self, x):
        """
        input tensor of dims [b, d], where d is the number of hidden features.
        return tensor of dims [b, c, h, w] of type float16 (will be cast to float16 if not)
        """

        b = x.shape[0]

        x = torch.relu(self.fc(x))
        x = x.view((b, *self.final_dims))
        x = torch.max_pool2d(x, (2, 2), (2, 2))
        x = torch.relu(self.deconv1(x))
        x = torch.max_pool2d(x, (2, 2), (2, 2))
        x = torch.relu(self.deconv2(x))
        x = torch.sigmoid(self.deconv3(x))

        return x

class CentralizedCritic():
    """
    This model takes as input one global state and provides the following outputs for each player

        Feature Extractor -> Extrinsic Value
                          -> Intrinsic Value

    It can be used when estimating returns to reduce variance during training. This need not be used once policys have
    been formed.
    """
    pass


class PMModel(nn.Module):

    """
    Multi-agent model for PPO Marl implementation

    This model provides the following outputs

    Feature Extractor -> LSTM -> Policy
                              -> Extrinsic Value (local estimation)
                              -> Intrinsic Value (local estimation)
                              -> Observation estimates for each player
                              -> Estimate of other players prediction of our own observation
    """

    def __init__(
            self,
            env: MultiAgentVecEnv,
            device="cpu",
            dtype=torch.float32,
            memory_units=128,
            out_features=128,
            model="default",
            data_parallel=False
    ):

        self.name = "PM"

        assert env.observation_space.dtype == np.uint8, "Observation space should be 8bit integer"

        super().__init__()
        self.input_dims = env.observation_space.shape
        self.actions = env.action_space.n
        self.device = device
        self.dtype = dtype

        if model.lower() == "default":
            self.encoder = DefaultEncoder(env.observation_space.shape, out_features=out_features)
        elif model.lower() == "fast":
            self.encoder = FastEncoder(env.observation_space.shape, out_features=out_features)
        else:
            raise Exception(f"Invalid model {model}, expected [default|fast]")

        if data_parallel:
            # enable multi gpu :)
            print(" - enabling multi-GPU support")
            self.encoder = nn.DataParallel(self.encoder)

        self.encoder_features = self.encoder.out_features
        self.memory_units = memory_units

        # note, agents are AI controlled, players maybe scripted, but an AI still needs to predict their behavour.
        self.n_agents = env.total_agents
        self.game_players = env.max_players
        self.n_roles = env.max_roles
        self.obs_shape = env.observation_space.shape

        # memory units
        self.lstm = torch.nn.LSTM(input_size=self.encoder_features, hidden_size=self.memory_units, num_layers=1,
                                  batch_first=False, dropout=0)

        # output heads
        self.policy_head = nn.Linear(self.memory_units, env.action_space.n)
        #torch.nn.init.xavier_uniform_(self.policy_head.weight, gain=0.01)  # this helps with model's initial exploration

        self.local_int_value_head = nn.Linear(self.memory_units, 1)
        self.local_ext_value_head = nn.Linear(self.memory_units, 1)

        # self.role_prediction_head = nn.Linear(self.memory_units, (self.n_players * self.n_roles))
        # self.observation_prediction_head = BasicDecoder((self.obs_shape[0]*self.n_players, *self.obs_shape[1:]),
        #                                                 self.memory_units)
        #
        # self.self_observation_prediction_head = BasicDecoder((self.obs_shape[0] * self.n_players, *self.obs_shape[1:]),
        #                                                      self.memory_units)

        self.set_device_and_dtype(self.device, self.dtype)

    def forward_sequence(self, obs, rnn_states, terminals=None):
        """
        Forward a sequence of observations through model, returns dictionary of outputs.
        :param obs: input observations, tensor of dims [N, B, observation_shape], which should be channels last. (BHWC)
        :param rnn_states: float32 tensor of dims [B, 2, memory_dims] containing the initial rnn h,c states
        :param terminals: (optional) tensor of dims [N, B] indicating transitions with a terminal state with a 1
        :return: tuple(
            dictionary containing log_policy, value_ext, and value_int
            new rnn_state
            ) all having structure [N, B, ...]

        """

        N, B, *obs_shape = obs.shape

        # merge first two dims into a batch, run it through encoder, then reshape it back into the correct form.
        assert tuple(obs_shape) == self.obs_shape
        assert tuple(rnn_states.shape) == (B, 2, self.memory_units)
        if terminals is not None:
            assert tuple(terminals.shape) == (N, B)

        if terminals is not None and terminals.sum() == 0:
            # just ignore terminals if none occurred (faster)
            terminals = None

        obs = obs.reshape([N*B, *obs_shape])
        obs = self.prep_for_model(obs)

        encoding = self.encoder(obs)
        encoding = encoding.reshape([N, B, -1])

        # torch requires these to be contiguous, it's a shame we have to do this but it keeps things simpler to use
        # one tensor rather than a separate one for h and c.
        # also, h,c should be dims [1, B, memory_units] as we have 1 LSTM layer and it is unidirectional
        h = rnn_states[np.newaxis, :, 0, :].clone().detach().contiguous()
        c = rnn_states[np.newaxis, :, 1, :].clone().detach().contiguous()

        if terminals is None:
            # this is the faster path if there are no terminals
            lstm_output, (h, c) = self.lstm(encoding, (h, c))
        else:
            # this is about 2x slower, and takes around 100ms on a batch size of 4096
            outputs = []
            for t in range(N):
                output, (h, c) = self.lstm(encoding[t:t+1], (h, c))

                if terminals is not None:
                    terminals_t = terminals[t][np.newaxis, :, np.newaxis]
                    h = h * (1.0 - terminals_t)
                    c = c * (1.0 - terminals_t)

                outputs.append(output)
            lstm_output = torch.cat(outputs, dim=0) # do we loose gradients this way?

        new_rnn_states = torch.zeros_like(rnn_states)

        new_rnn_states[:, 0, :] = h
        new_rnn_states[:, 1, :] = c

        # todo: do we need to role the first two dims together?
        # stub, I don't think this is necessary, put it back
        lstm_output = lstm_output.reshape(N*B, *lstm_output.shape[2:])

        log_policy = self.policy_head(lstm_output)
        log_policy = torch.log_softmax(log_policy, dim=1).reshape(N, B, self.actions)
        ext_value = self.local_ext_value_head(lstm_output).squeeze(dim=1).reshape(N, B)
        int_value = self.local_int_value_head(lstm_output).squeeze(dim=1).reshape(N, B)

        return {
            'log_policy': log_policy,
            'ext_value': ext_value,
            'int_value': int_value
        }, new_rnn_states


    # def __get_role_prediction(self):
    #     """
    #     Give roles prediction for each player as a distribution over roles
    #     :return: tensor of dims [b, n, r] representing log prob for each role for each player
    #     """
    #
    #     predictions = self.role_prediction_head(self.hidden_features)
    #     predictions = predictions.reshape((self.n_agents, self.n_players, self.n_roles))
    #     predictions = torch.log_softmax(predictions, dim=2)
    #     return predictions
    #
    # def __get_observation_prediction(self):
    #     """
    #     Predict observations for all players for each other player
    #     :return: float16 tensor of dims [b, p, c, h, w], where out[i, j, ...] is i's prediction about j's observation
    #     """
    #
    #     prediction = self.observation_prediction_head(self.hidden_features)
    #     prediction = prediction.view((self.n_agents, self.n_players, *self.obs_shape))
    #     return prediction
    #
    # def __get_self_observation_prediction(self):
    #     """
    #     Predict other players predictions of our current observation
    #     :return: float16 tensor of dims [b, p, c, h, w], where out[i, j, ...] is i's prediction about j's observation
    #     """
    #
    #     prediction = self.self_observation_prediction_head(self.hidden_features)
    #     prediction = prediction.view((self.n_agents, self.n_players, *self.obs_shape))
    #     return prediction

    def prep_for_model(self, x, scale_int=True):
        """ Converts data to format for model (i.e. uploads to GPU, converts type).
            Can accept tensor or ndarray.
            scale_int scales uint8 to [0..1]
         """

        assert self.device is not None, "Must call set_device_and_dtype."

        utils.validate_dims(x, (None, *self.input_dims))

        # if this is numpy convert it over
        if type(x) is np.ndarray:
            x = torch.from_numpy(x)

        # move it to the correct device
        x = x.to(device=self.device, non_blocking=True)

        # then covert the type (faster to upload uint8 then convert on GPU)
        if x.dtype == torch.uint8:
            x = x.to(dtype=self.dtype, non_blocking=True)
            if scale_int:
                x = x / 255
        elif x.dtype == self.dtype:
            pass
        else:
            raise Exception("Invalid dtype {}".format(x.dtype))

        return x

    def set_device_and_dtype(self, device, dtype):

        self.to(device)
        if str(dtype) in ["torch.half", "torch.float16"]:
            self.half()
        elif str(dtype) in  ["torch.float", "torch.float32"]:
            self.float()
        elif str(dtype) in ["torch.double", "torch.float64"]:
            self.double()
        else:
            raise Exception("Invalid dtype {} for model.".format(dtype))
        self.device, self.dtype = device, dtype


class MarlAlgorithm():

    def __init__(self, N, A, model: nn.Module):

        self.model = model

        self.batch_size = int()
        self.n_steps = N
        self.n_agents = A

        self.obs_shape = self.model.input_dims
        self.rnn_state_shape = [2, self.model.memory_units]  # records h and c for LSTM units.
        self.policy_shape = [self.model.actions]

    def learn(self, total_timesteps, reset_num_timesteps=True):
        raise NotImplemented()

    def step(self, obs):
        raise NotImplemented()

    @staticmethod
    def load(filename, env):
        raise NotImplemented()

    def get_initial_state(self, n_agents=None):
        n_agents = n_agents or self.n_agents
        return np.zeros([n_agents, *self.rnn_state_shape])

class PMAlgorithm(MarlAlgorithm):
    """
    Handles rollout generation and training on rollout data.
    """

    def __init__(
            self,
            vec_env:MultiAgentVecEnv,  # the environment this agent acts in

            amp=False,                 # automatic mixed precision
            name="agent",              # name of agent
            verbose=False,             # set to true to enable debug printing

            # ------ model parameters ------------

            data_parallel=False,
            device="cpu",
            memory_units=128,
            out_features=128,
            model_name="default",
            micro_batches:Union[str, int]="auto",

            # ------ long list of parameters to algorithm ------------
            n_steps=128,
            learning_rate=2.5e-4,
            adam_epsilon=1e-5,
            normalize_advantages=True,
            gamma=0.995,
            gamma_int=0.995,
            gae_lambda=0.95,
            batch_epochs=4,
            entropy_bonus=0.01,
            mini_batches=8,  # stub was 4
            use_clipped_value_loss=False,  # shown to be not effective
            vf_coef=0.5,
            ppo_epsilon=0.2,
            max_grad_norm=0.5,
        ):

        A = vec_env.total_agents
        N = n_steps

        model = PMModel(vec_env, device=device, memory_units=memory_units, model=model_name,
                        data_parallel=data_parallel, out_features=out_features)
        super().__init__(n_steps, A, model)

        self.name = name
        self.amp = amp
        self.verbose = verbose
        self.log = Logger()
        self.vec_env = vec_env

        self.normalize_advantages = normalize_advantages
        self.gamma = gamma
        self.gamma_int = gamma_int
        self.gae_lambda = gae_lambda
        self.batch_epochs = batch_epochs
        self.entropy_bonus = entropy_bonus
        self.mini_batches = mini_batches
        self.use_clipped_value_loss = use_clipped_value_loss
        self.vf_coef = vf_coef
        self.ppo_epsilon = ppo_epsilon
        self.max_grad_norm = max_grad_norm

        # these are not used yet
        self.intrinsic_reward_propagation = False,
        self.normalize_intrinsic_rewards = False
        self.extrinsic_reward_scale = 1.0
        self.intrinsic_reward_scale = 1.0
        self.log_folder = "."
        self.deception_bonus = False  # this is not implemented yet

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, eps=adam_epsilon)

        self.t = 0
        self.learn_steps = 0
        self.batch_size = N * A

        self.episode_score = np.zeros([A], dtype=np.float32)
        self.episode_len = np.zeros([A], dtype=np.int32)

        # ------------------------------------
        # current agent state

        # agents most previously seen observation
        self.agent_obs = np.zeros([A, *self.obs_shape], dtype=np.uint8)
        # agents current rnn state
        self.agent_rnn_state = torch.zeros([A, *self.rnn_state_shape], dtype=torch.float32, device=self.model.device)

        # ------------------------------------
        # the rollout transitions (batch)

        # rnn_state of the agent before rollout begins
        self.prev_rnn_state = np.zeros([A, *self.rnn_state_shape], dtype=np.float32)
        # observation of the agent before taking action
        self.prev_obs = np.zeros([N, A, *self.obs_shape], dtype=np.uint8)
        # observation of the agent after taking action
        self.next_obs = np.zeros([N, A, *self.obs_shape], dtype=np.uint8) # note, do I need these?
        # action taken by agent to generate the transition
        self.actions = np.zeros([N, A], dtype=np.int64)
        # log_policy that generated the action
        self.log_policy = np.zeros([N, A, *self.policy_shape], dtype=np.float32)
        # float, 1 indicates action resulted in a terminal state, 0 indicates non-terminal state
        self.terminals = np.zeros([N, A], dtype=np.float32) # 1 if given time step was a terminal timestep for given agent.
        # learning rate weight, set to 0 to ignore a transition (i.e. if the player is dead)
        self.learning_weight = np.zeros([N, A], dtype=np.float32)
        # rewards generated at transition
        self.ext_rewards = np.zeros([N, A], dtype=np.float32)
        self.int_rewards = np.zeros([N, A], dtype=np.float32)
        # value estimates by agent just before action was taken
        self.ext_value = np.zeros([N, A], dtype=np.float32)
        self.int_value = np.zeros([N, A], dtype=np.float32)
        # return estimates (from which state though?)
        self.ext_returns = np.zeros([N, A], dtype=np.float32)
        self.int_returns = np.zeros([N, A], dtype=np.float32)
        self.int_returns_raw = np.zeros([N, A], dtype=np.float32)

        # advantage used during policy gradient (combination of intrinsic and extrinsic reward)
        self.advantage = np.zeros([N, A], dtype=np.float32)

        self.ext_final_value_estimate = np.zeros([A], dtype=np.float32)
        self.int_final_value_estimate = np.zeros([A], dtype=np.float32)

        self.intrinsic_returns_rms = utils.RunningMeanStd(shape=())

        # get micro batch size
        self.micro_batches = 0

        if type(micro_batches) is int:
            self.micro_batches = micro_batches
        elif micro_batches.lower() == "auto":
            self.micro_batches = self._get_auto_micro_batch_size()
        elif micro_batches.lower() == "off":
            self.micro_batches = 0
        else:
            raise ValueError(f"Invalid micro batches parameter {micro_batches}")

        if self.micro_batches != 1:
            print(f" -using micro_batches={self.micro_batches}")

        self.scaler = GradScaler() if self.amp else None

    @property
    def mini_batch_size(self):
        return self.batch_size // self.mini_batches

    def learn(self, total_timesteps, reset_num_timesteps=True):

        if reset_num_timesteps:
            self.t = 0
            self.reset()
            self.learn_steps = 0

        batches = total_timesteps // self.batch_size
        for batch in range(batches):
            self.model.eval()
            self.generate_rollout()
            self.model.train()
            self.train()
            self.model.eval()
            if self.verbose:
                if self.learn_steps % 10 == 0:
                    print(f"{self.learn_steps}, {self.t / 1000 / 1000:.1f}M")
                self.log.print_variables(include_header=self.learn_steps % 10 == 0)
            self.t += self.batch_size
            self.learn_steps += 1
            self.log.record_step()

    def _get_auto_micro_batch_size(self):
        """ Returns an appropriate microbatch size. """

        # working this out will be complex
        # maybe can assume that 12GB gives us 4096 with AMP and 2048 without
        if type(self.model.encoder) is DefaultEncoder:
            max_micro_batch_size = 2048
        elif type(self.model.encoder) is FastEncoder:
            max_micro_batch_size = 8192
        else:
            max_micro_batch_size = 1024 # just a guess

        if self.amp:
            max_micro_batch_size *= 2
        if self.data_parallel:
            max_micro_batch_size *= 4 # should be number of GPUs

        return max(1, self.mini_batch_size // max_micro_batch_size)

    def reset(self):
        # initialize agent
        self.agent_obs = self.vec_env.reset()
        self.agent_rnn_state *= 0
        self.episode_score *= 0
        self.episode_len *= 0

    def step(self, obs):
        """
        Applies one agent step, returning actions, value, new rnn states, log_policy, and updating rnn_state
        :param obs: tensor of dims [A, *observation_shape]
        :return: (actions, value, log_policy) as np arrays
        """

        with torch.no_grad():
            model_output = self.forward(obs)
            log_policy = model_output["log_policy"].detach().cpu().numpy()
            value = model_output["ext_value"].detach().cpu().numpy()
            actions = utils.sample_action_from_logp(log_policy)

        return actions, value, log_policy

    def generate_rollout(self):
        """
        Runs agents through environment for n_steps steps, recording the transitions as it goes.
        :return: Nothing
        """
        with torch.no_grad():

            self.prev_rnn_state = self.agent_rnn_state.clone().detach()

            for t in range(self.n_steps):

                prev_obs = self.agent_obs.copy()

                learning_weight = self.vec_env.get_alive()

                # forward state through model, then detach the result and convert to numpy.
                model_out = self.forward(self.agent_obs)

                log_policy = model_out["log_policy"].detach().cpu().numpy()
                ext_value = model_out["ext_value"].detach().cpu().numpy()

                # sample actions and run through environment.
                actions = np.asarray([utils.sample_action_from_logp(prob) for prob in log_policy], dtype=np.int32)
                self.agent_obs, ext_rewards, terminals, infos = self.vec_env.step(actions)

                # work out our intrinsic rewards
                int_value = model_out["int_value"].detach().cpu().numpy()
                int_rewards = np.zeros_like(ext_rewards)

                # save raw rewards for monitoring the agents progress
                raw_rewards = np.asarray([info.get("raw_reward", reward) for reward, info in zip(ext_rewards, infos)],
                                         dtype=np.float32)

                self.episode_score += raw_rewards
                self.episode_len += 1

                for i, terminal in enumerate(terminals):
                    if terminal:
                        # reset the internal state on episode completion
                        self.agent_rnn_state[i] *= 0
                        # reset is handled automatically by vectorized environments
                        # so just need to keep track of book-keeping
                        self.log.watch_full("ep_score", self.episode_score[i])
                        self.log.watch_full("ep_length", self.episode_len[i])
                        self.episode_score[i] = 0
                        self.episode_len[i] = 0

                self.prev_obs[t] = prev_obs
                self.next_obs[t] = self.agent_obs
                self.actions[t] = actions

                self.ext_rewards[t] = ext_rewards
                self.log_policy[t] = log_policy
                self.terminals[t] = terminals
                self.learning_weight[t] = learning_weight
                self.int_rewards[t] = int_rewards
                self.ext_value[t] = ext_value
                self.int_value[t] = int_value

            # get value estimates for final observation.
            model_out = self.forward(self.agent_obs, update_rnn_state=False)

            self.ext_final_value_estimate = model_out["ext_value"].detach().cpu().numpy()
            if "int_value" in model_out:
                self.int_final_value_estimate = model_out["int_value"].detach().cpu().numpy()

        self.calculate_returns()

    def calculate_returns(self):
        """
        Calculate returns from previous rollout.
        Updates the following variables
            ext_returns, int_returns
            ext_advantage
        :return:
        """

        self.ext_returns = calculate_returns(self.ext_rewards, self.terminals, self.ext_final_value_estimate,
                                             self.gamma)

        self.ext_advantage = calculate_gae(self.ext_rewards, self.ext_value, self.ext_final_value_estimate,
                                           self.terminals, self.gamma, self.gae_lambda, self.normalize_advantages)

        # calculate the intrinsic returns, but let returns propagate through terminal states.
        self.int_returns_raw = calculate_returns(
            self.int_rewards,
            self.intrinsic_reward_propagation * self.terminals,
            self.int_final_value_estimate,
            self.gamma_int
        )

        if self.normalize_intrinsic_rewards:
            # normalize returns using EMS
            for t in range(self.n_steps):
                self.ems_norm = 0.99 * self.ems_norm + self.int_rewards[t, :]
                self.intrinsic_returns_rms.update(self.ems_norm.reshape(-1))
            # normalize the intrinsic rewards
            # we multiply by 0.4 otherwise the intrinsic returns sit around 1.0, and we want them to be more like 0.4,
            # which is approximately where normalized returns will sit.
            self.intrinsic_reward_norm_scale = (1e-5 + self.intrinsic_returns_rms.var ** 0.5)
            self.batch_int_rewards = self.int_rewards / self.intrinsic_reward_norm_scale * 0.4
        else:
            self.intrinsic_reward_norm_scale = 1
            self.batch_int_rewards = self.int_rewards

        self.int_returns = calculate_returns(
            self.int_rewards,
            self.intrinsic_reward_propagation * self.terminals,
            self.int_final_value_estimate,
            self.gamma_int
        )

        self.int_advantage = calculate_gae(self.int_rewards, self.int_value, self.int_final_value_estimate, None,
                                           self.gamma_int)

        self.advantage = self.extrinsic_reward_scale * self.ext_advantage + self.intrinsic_reward_scale * self.int_advantage

        self.log.watch_mean("adv_mean", np.mean(self.advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_std", np.std(self.advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_max", np.max(self.advantage), display_width=10 if self.normalize_advantages else 0)
        self.log.watch_mean("adv_min", np.min(self.advantage), display_width=10 if self.normalize_advantages else 0)
        self.log.watch_mean("batch_reward_ext", np.mean(self.ext_rewards), display_name="rew_ext", display_width=0)
        self.log.watch_mean("batch_return_ext", np.mean(self.ext_returns), display_name="ret_ext")
        self.log.watch_mean("batch_return_ext_std", np.std(self.ext_returns), display_name="ret_ext_std",
                            display_width=0)
        self.log.watch_mean("value_est_ext", np.mean(self.ext_value), display_name="est_v_ext")
        self.log.watch_mean("value_est_ext_std", np.std(self.ext_value), display_name="est_v_ext_std", display_width=0)
        self.log.watch_mean("ev_ext", utils.explained_variance(self.ext_value.ravel(), self.ext_returns.ravel()))

        if self.deception_bonus:
            self.log.watch_mean("batch_reward_int", np.mean(self.int_rewards), display_name="rew_int", display_width=0)
            self.log.watch_mean("batch_reward_int_std", np.std(self.int_rewards), display_name="rew_int_std",
                                display_width=0)
            self.log.watch_mean("batch_return_int", np.mean(self.int_returns), display_name="ret_int")
            self.log.watch_mean("batch_return_int_std", np.std(self.int_returns), display_name="ret_int_std")
            self.log.watch_mean("batch_return_int_raw_mean", np.mean(self.int_returns_raw),
                                display_name="ret_int_raw_mu",
                                display_width=0)
            self.log.watch_mean("batch_return_int_raw_std", np.std(self.int_returns_raw),
                                display_name="ret_int_raw_std",
                                display_width=0)

            self.log.watch_mean("value_est_int", np.mean(self.int_value), display_name="est_v_int")
            self.log.watch_mean("value_est_int_std", np.std(self.int_value), display_name="est_v_int_std")
            self.log.watch_mean("ev_int", utils.explained_variance(self.int_value.ravel(), self.int_returns.ravel()))

            if self.normalize_intrinsic_rewards:
                self.log.watch_mean("norm_scale_int", self.intrinsic_reward_norm_scale, display_width=12)

    def forward(self, obs, update_rnn_state=True):
        """
        Forwards a single observations through model for each agent, updating the agents rnn_state.
        Any number of observations <= self.n_agents can be used, but only those rnn_states will be updated.
        For more advanced uses call model.forward_sequence directly.

        :param obs: Observations for each agent, tensor of dims [B, *observation_shape]
        :param update_rnn_state: If true RNN states for agents will be updated with the new state information

        :return: a dictionary containing,
            'rnn_state',        [B, 2, memory_dims]
            'log_policy',       [B, actions]
            'ext_value', and    [B]
            'int_value'.        [B]
        """

        B = len(obs)

        # get a copy of the states (the copy probably isn't needed here, but I do it for safety as forward_sequence
        # used to modify them in place, if this is fixed this can be put back to a non-copy operation
        agent_rnn_states = self.agent_rnn_state[:B].clone().detach()

        # make a sequence of length 1 and forward it through model
        result, new_rnn_states = self.model.forward_sequence(obs[np.newaxis], agent_rnn_states)

        # remove the sequence of length 1
        unsqueze_result = {}
        for k, v in result.items():
            unsqueze_result[k] = v[0]

        if update_rnn_state:
            # update only the states which changed
            self.agent_rnn_state[:B] = new_rnn_states

        return unsqueze_result

    def _merge_down(self, x):
        """ Combines first two dims. """
        return x.reshape(-1, *x.shape[2:])

    def forward_mini_batch(self, data, rnn_states):
        """
        Run mini batch through model generating loss, call opt_step_mini_batch to apply the update.
        mini batch should have the structure [N, B, *] where N is the sequence length, and is the batch length

        :param data: dictionary containing data for
        :param rnn_states: current rnn_states of dims [B, 2, memory_units]
        """

        loss = torch.tensor(0, dtype=torch.float32, device=self.model.device)

        # -------------------------------------------------------------------------
        # Calculate loss_pg
        # -------------------------------------------------------------------------

        prev_obs = data["prev_obs"]
        terminals = data["terminals"]

        # these were provided in N, B format but we want them just as one large batch.
        actions = self._merge_down(data["actions"])
        policy_log_probs = self._merge_down(data["log_policy"])
        advantages = self._merge_down(data["advantages"])
        weights = self._merge_down(data["learning_weight"]) if "learning_weight" in data else 1

        N, B, *obs_shape = prev_obs.shape

        model_out, _ = self.model.forward_sequence(prev_obs, rnn_states, terminals)

        # output will be a sequence of [N, B] but we want this just as [N*B] for processing
        for k, v in model_out.items():
            model_out[k] = self._merge_down(v)

        logps = model_out["log_policy"]

        ratio = torch.exp(logps[range(N*B), actions] - policy_log_probs[range(N*B), actions])
        clipped_ratio = torch.clamp(ratio, 1 - self.ppo_epsilon, 1 + self.ppo_epsilon)

        loss_clip = torch.min(ratio * advantages, clipped_ratio * advantages)
        loss_clip_mean = (weights*loss_clip).mean()

        self.log.watch_mean("loss_pg", loss_clip_mean, history_length=64)
        loss += loss_clip_mean

        # -------------------------------------------------------------------------
        # Calculate loss_value
        # -------------------------------------------------------------------------

        value_heads = ["ext", "int"]

        for value_head in value_heads:
            value_prediction = model_out["{}_value".format(value_head)]
            returns = self._merge_down(data["{}_returns".format(value_head)])
            old_pred_values = self._merge_down(data["{}_value".format(value_head)])

            if self.use_clipped_value_loss:
                # is is essentially trust region for value learning, and seems to help a lot.
                value_prediction_clipped = old_pred_values + torch.clamp(value_prediction - old_pred_values,
                                                                         -self.ppo_epsilon, +self.ppo_epsilon)
                vf_losses1 = (value_prediction - returns).pow(2)
                vf_losses2 = (value_prediction_clipped - returns).pow(2)
                loss_value = -self.vf_coef * torch.mean(torch.max(vf_losses1, vf_losses2) * weights)
            else:
                # simpler version, just use MSE.
                vf_losses1 = (value_prediction - returns).pow(2)
                loss_value = -self.vf_coef * torch.mean(vf_losses1 * weights)

            # chekc is it self.vf_coef * ... or 0.5 * self.vf_coef ...

            self.log.watch_mean("loss_v_" + value_head, loss_value, history_length=64)
            loss += loss_value

        # -------------------------------------------------------------------------
        # Calculate loss_entropy
        # -------------------------------------------------------------------------

        loss_entropy = -(logps.exp() * logps).sum(axis=1)
        loss_entropy = (loss_entropy * weights * self.entropy_bonus).mean()
        self.log.watch_mean("loss_ent", loss_entropy)
        loss += loss_entropy

        self.log.watch_mean("loss", loss)
        loss = loss / self.micro_batches

        # -------------------------------------------------------------------------
        # Run optimizer
        # -------------------------------------------------------------------------

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(-loss).backward()
        else:
            (-loss).backward()

    def _log_and_clip_grad_norm(self):
        if self.max_grad_norm is not None and self.max_grad_norm != 0:
            grad_norm = nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
        else:
            # even if we don't clip the gradient we should at least log the norm. This is probably a bit slow though.
            # we could do this every 10th step, but it's important that a large grad_norm doesn't get missed.
            grad_norm = 0
            parameters = list(filter(lambda p: p.grad is not None, self.model.parameters()))
            for p in parameters:
                param_norm = p.grad.data.norm(2)
                grad_norm += param_norm.item() ** 2
            grad_norm = grad_norm ** 0.5
        self.log.watch_mean("opt_grad", grad_norm)

    def opt_step_minibatch(self):
        """ Runs the optimization step for a minimatch, forward_minibatch should be called first."""

        if self.amp:
            self.scaler.unscale_(self.optimizer)
            self._log_and_clip_grad_norm()
            self.scaler.step(self.optimizer)
            self.scaler.update()
            self.optimizer.zero_grad()
        else:
            self._log_and_clip_grad_norm()
            self.optimizer.step()
            self.optimizer.zero_grad()


    def train(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        batch_data = {}

        batch_data["prev_obs"] = self.prev_obs
        batch_data["actions"] = self.actions.astype(np.long)
        batch_data["ext_returns"] = self.ext_returns
        batch_data["int_returns"] = self.int_returns
        batch_data["ext_value"] = self.ext_value
        batch_data["int_value"] = self.int_value
        batch_data["log_policy"] = self.log_policy
        batch_data["advantages"] = self.advantage
        batch_data["terminals"] = self.terminals

        # stub: disable learning weights
        # this should be for any player that was dead at the start of this transition (but not players who died
        # during this transition
        # batch_data["learning_weight"] = self.learning_weight


        for i in range(self.batch_epochs):

            assert self.mini_batch_size % self.n_steps == 0, "mini batch size must be multiple of n_steps."

            segments = list(range(self.n_agents))
            np.random.shuffle(segments)

            # figure out how many agents to run in parallel so that mini batch size is right
            # this will be num_segments * rnn_block_length

            assert self.mini_batch_size % self.micro_batches == 0

            segments_per_micro_batch = self.batch_size // self.n_steps // \
                                       (self.mini_batches * self.micro_batches)

            self.optimizer.zero_grad()

            micro_batch_counter = 0

            for j in range(self.mini_batches):
                with autocast(enabled=self.amp):
                    for k in range(self.micro_batches):
                        batch_start = micro_batch_counter * segments_per_micro_batch
                        batch_end = (micro_batch_counter + 1) * segments_per_micro_batch
                        sample = segments[batch_start:batch_end]

                        # initialize states from state taken during rollout, then upload to gpu
                        rnn_states = self.prev_rnn_state[sample].to(device=self.model.device)

                        # put together a mini batch from all agents at this time step...
                        mini_batch_data = {}
                        for key, value in batch_data.items():
                            mini_batch_data[key] = torch.tensor(value[:, sample], device=self.model.device)

                        self.forward_mini_batch(mini_batch_data, rnn_states=rnn_states)
                        micro_batch_counter += 1
                self.opt_step_minibatch()

# ------------------------------------------------------------------
# Helper functions
# ------------------------------------------------------------------

def calculate_returns(rewards, dones, final_value_estimate, gamma):
    """
    Calculates returns given a batch of rewards, dones, and a final value estimate.
    Input is vectorized so it can calculate returns for multiple agents at once.
    :param rewards: nd array of dims [N,A]
    :param dones:   nd array of dims [N,A] where 1 = done and 0 = not done.
    :param final_value_estimate: nd array [A] containing value estimate of final state after last action.
    :param gamma:   discount rate.
    :return:
    """

    N, A = rewards.shape

    returns = np.zeros([N, A], dtype=np.float32)
    current_return = final_value_estimate

    for i in reversed(range(N)):
        returns[i] = current_return = rewards[i] + current_return * gamma * (1.0 - dones[i])

    return returns

def calculate_gae(batch_rewards, batch_value, final_value_estimate, batch_terminal, gamma, lamb=1.0, normalize=False):

    N, A = batch_rewards.shape

    batch_advantage = np.zeros_like(batch_rewards, dtype=np.float32)
    prev_adv = np.zeros([A], dtype=np.float32)
    for t in reversed(range(N)):
        is_next_terminal = batch_terminal[
            t] if batch_terminal is not None else False  # batch_terminal[t] records if t+1 is a terminal state)
        value_next_t = batch_value[t + 1] if t != N - 1 else final_value_estimate
        delta = batch_rewards[t] + gamma * value_next_t * (1.0 - is_next_terminal) - batch_value[t]
        batch_advantage[t] = prev_adv = delta + gamma * lamb * (
                1.0 - is_next_terminal) * prev_adv
    if normalize:
        batch_advantage = (batch_advantage - batch_advantage.mean()) / (batch_advantage.std() + 1e-8)
    return batch_advantage

def prod(X):
    """
    Return the product of elements in X
    :param X:
    :return:
    """
    y = 1
    for x in X:
        y *= x
    return y
