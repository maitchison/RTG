"""
PPO implementation for Multi-Agent Reinforcement Learning

Planned Features

[ ] Role Prediction
[ ] Prediction of other players observations
[ ] Prediction of other players prediction of our own observation
[ ] Optional Shared value function during training
[ ] micro batching for low GPU machines (using gradient accumulation)

Notation

b: number of agents calculated simultaneously (effectively the batch size used during rollout)
p: (max) number of players in game
r: number of roles
a: number of actions

batch: the total batch of data generated by a rollout, i.e. 64k
minibatch: the batch size used during optimizer updates, often batch/4, e.g. 16k
microbatch: smaller batches used due to GPU memory limitations, gradients are accumulated during microbatch processing
    making them equivalent to a larger minibatch size, but with less memory (this does not allow for batch-normalizion
    however)

extrinsic and intrinsic rewards are learned separately as value_int, and value_ext

"""

import torch.nn.functional as F
import pickle
import gzip
import time
import os

from torch.utils.tensorboard import SummaryWriter

from torch.cuda.amp import GradScaler, autocast
from logger import Logger
from typing import Union

from utils import Color as C
from utils import check_tensor

from models import *

from marl_env import MultiAgentVecEnv
import torch.autograd.profiler as profiler

import platform

# this enable text colors on windows
if platform.system() == "Windows":
    import colorama
    colorama.init()

class MarlAlgorithm():

    def __init__(self, N, A, model: PolicyModel):

        self.policy_model = model

        self.batch_size = int()
        self.n_steps = N
        self.n_agents = A

        self.obs_shape = self.policy_model.input_shape
        self.rnn_state_shape = [2, self.policy_model.memory_units]  # records h and c for LSTM units
        self.policy_shape = [self.policy_model.n_actions]

    def learn(self, total_timesteps, reset_num_timesteps=True):
        raise NotImplemented()

    def save_logs(self, base_path):
        raise NotImplemented()

    @staticmethod
    def load(filename, env):
        raise NotImplemented()

    def get_initial_rnn_state(self, n_agents=None):
        n_agents = n_agents or self.n_agents
        return torch.zeros([n_agents, *self.rnn_state_shape], dtype=torch.float32, device=self.policy_model.device)

    def forward(self, obs, rnn_states, roles=None):
        raise NotImplemented()

class PMAlgorithm(MarlAlgorithm):
    """
    Handles rollout generation and training on rollout data.
    """

    DM_OFF = "off"
    DM_ACTION = "action"
    DM_OBSERVATION = "observation"
    DM_BOTH = "both"

    def __init__(
            self,
            vec_env:MultiAgentVecEnv,  # the environment this agent acts in
            name="agent",              # name of agent
            verbose=False,             # set to true to enable debug printing
            export_rollout=False,      # writes rollouts to disk (very large!)

            # ------ model parameters ------------

            amp=False,  # automatic mixed precision
            device="cuda",
            policy_memory_units=128,   # >128 is required if we want to predict roles using policy module
            model_name="default",
            micro_batch_size: Union[str, int] = "auto",
            # the type of prediction to use for deception.  Observation is much slower to process
            prediction_mode="off",   # off|action|observation|both
            split_policy=False,         # uses separate models for each role (slower)

            # ------ long list of parameters to algorithm ------------
            n_steps=32,
            learning_rate=2.5e-4,
            adam_epsilon=1e-8,
            normalize_advantages=True,
            gamma=0.99,
            gamma_int=0.99,
            gae_lambda=0.95,
            batch_epochs=4,
            entropy_bonus=0.01,
            mini_batch_size=2048,
            use_clipped_value_loss=False,  # shown to be not effective
            vf_coef=0.5,
            ppo_epsilon=0.2,
            max_grad_norm=5.0,
            lstm_mode="residual",
            deception_bonus: tuple = (0,0,0),            # scale of deception bonus for each team (requires deception module to be enabled)
            deception_bonus_start_timestep: int = 10e6,  # deception bonus will be 0 until this timestep.
            use_global_value_module=False,  # enables a global value function estimator, which improves training

            nan_check=False,                # if enabled checks for nans, and logs extreme values (slows things down)
            max_window_size:Union[int, None]=None,

            # ------ deception module settings ----------------

            dm_replay_buffer_multiplier=1,  # this doesn't seem to help... so keep it at 1
            dm_max_window_size=8,
            dm_mini_batch_size=256,         # 128 works best, but train's 20% slower than 256, which is also ok.
            dm_memory_units=512,
            dm_out_features=512,
            dm_learning_rate=2.5e-4,
            dm_lstm_mode="residual",
            dm_kl_factor=0,                 # 1 = train on KL only, 0 = loss_fn only, and 0.5 is a 50/50 mixture
            dm_vision_filter=0.25,          # what proportion of non_visible agent pairs to train on
            dm_loss_scale=0.1,              # loss scale for deception module
        ):

        if type(deception_bonus) in [float, int]:
            deception_bonus = (deception_bonus, deception_bonus, deception_bonus)

        assert prediction_mode in [self.DM_OFF, self.DM_ACTION, self.DM_OBSERVATION, self.DM_BOTH]
        if min(deception_bonus) == max(deception_bonus) != 0:
            assert prediction_mode != self.DM_OFF, "You must set prediction_mode"

        A = vec_env.total_agents
        N = n_steps
        R = 3 # stub locked to 3 roles for the moment

        if nan_check:
            print(f"[DEBUG: Using {C.WARNING}nan_check{C.ENDC}]")

        self.export_rollout = export_rollout

        # deception module settings
        self.dm_replay_buffer_multiplier = dm_replay_buffer_multiplier
        self.dm_max_window_size = dm_max_window_size
        self.dm_mini_batch_size = dm_mini_batch_size
        self.dm_memory_units = dm_memory_units
        self.dm_out_features = dm_out_features
        self.dm_lstm_mode = dm_lstm_mode
        self.dm_learning_rate = dm_learning_rate
        self.dm_loss_scale = dm_loss_scale
        self.dm_kl_factor = dm_kl_factor
        self.deception_bonus_start_timestep = deception_bonus_start_timestep
        self.deception_batch_counter = 0
        self.policy_batch_counter = 0
        self.gv_batch_counter = 0
        self.max_window_size = max_window_size

        # the type of prediction to use for deception off|action|observation|both
        self.prediction_mode = prediction_mode

        self.dm_vision_filter = dm_vision_filter

        if device == "data_parallel":
            device = "cuda"
            data_parallel = True
        else:
            data_parallel = False

        self.split_policy = split_policy
        policy_fn = SplitPolicyModel if self.split_policy else PolicyModel

        model = policy_fn(vec_env, device=device, memory_units=policy_memory_units, model=model_name,
                            data_parallel=data_parallel, out_features=policy_memory_units,
                            lstm_mode=lstm_mode, nan_check=nan_check)

        if use_global_value_module:
            print("Enabeling Global Value Module.")
            self.gv_model = GlobalValueModel(
                vec_env, device=device, memory_units=policy_memory_units, model=model_name,
                data_parallel=data_parallel, out_features=policy_memory_units,
                lstm_mode=lstm_mode, nan_check=nan_check
            )
        else:
            self.gv_model = None

        if self.uses_deception_model:
            self.deception_model = DeceptionModel(
                vec_env,
                n_predictions=vec_env.max_players,
                device=device,
                memory_units=dm_memory_units,
                out_features=dm_out_features,
                model=model_name,
                data_parallel=data_parallel,
                lstm_mode=self.dm_lstm_mode,
                predict='full',
                predict_observations=self.prediction_mode in ["both", "observation"],
                predict_actions=self.prediction_mode in ["both", "action"],
                nan_check=nan_check
            )
        else:
            self.deception_model = None

        super().__init__(n_steps, A, model)

        assert self.obs_shape[0] == 3, f"Model assumes RGB 3-channel input, but found {self.obs_shape}"

        states_needed = policy_memory_units

        if self.uses_deception_model:
            # to handle the deception rnn states just concatenate the h,c states together
            states_needed += dm_memory_units
        if use_global_value_module:
            # same goes for global value module
            states_needed += policy_memory_units

        self.rnn_state_shape = [2, states_needed]

        self.name = name
        self.amp = amp
        self.data_parallel = data_parallel
        self.verbose = verbose
        self.log = Logger()
        self.vec_env = vec_env
        self.policy_memory_units = policy_memory_units
        self.nan_check = nan_check

        self.normalize_advantages = normalize_advantages
        self.gamma = gamma
        self.gamma_int = gamma_int
        self.gae_lambda = gae_lambda
        self.batch_epochs = batch_epochs
        self.entropy_bonus = entropy_bonus
        self.mini_batch_size = mini_batch_size
        self.use_clipped_value_loss = use_clipped_value_loss
        self.vf_coef = vf_coef
        self.ppo_epsilon = ppo_epsilon
        self.max_grad_norm = max_grad_norm
        self.deception_bonus = deception_bonus

        self.intrinsic_reward_propagation = False
        self.normalize_intrinsic_rewards = True
        self.extrinsic_reward_scale = 1.0
        self.intrinsic_reward_scale = max(deception_bonus)
        self.log_folder = "."

        self.policy_optimizer = torch.optim.Adam(self.policy_model.parameters(), lr=learning_rate, eps=adam_epsilon)

        if self.uses_gv_model:
            self.gv_optimizer = torch.optim.Adam(self.gv_model.parameters(), lr=learning_rate, eps=adam_epsilon)

        if self.deception_model is not None:
            # optimal settings for deception optimizer are different than the policy optimizer
            self.deception_optimizer = torch.optim.Adam(self.deception_model.parameters(), lr=self.dm_learning_rate,
                                                        eps=1e-8)

        self.t = 0
        self.learn_steps = 0
        self.batch_size = N * A
        n_players = self.vec_env.max_players
        n_roles = 3

        self.episode_score = np.zeros([A], dtype=np.float32)
        self.episode_len = np.zeros([A], dtype=np.int32)

        # ------------------------------------
        # current agent state

        # agents most previously seen observation
        self.agent_obs = np.zeros([A, *self.obs_shape], dtype=np.uint8)
        # agents current rnn state
        self.agent_rnn_state = torch.zeros([A, *self.rnn_state_shape], dtype=torch.float32, device=self.policy_model.device)

        # ------------------------------------
        # the rollout transitions (batch)

        # rnn_state of the agent at each timestep
        self.batch_rnn_states = np.zeros([N, A, *self.rnn_state_shape], dtype=np.float32)
        # observation of the agent before taking action
        self.batch_prev_obs = np.zeros([N, A, *self.obs_shape], dtype=np.uint8)
        # action taken by agent to generate the transition
        self.batch_actions = np.zeros([N, A], dtype=np.int64)
        # log_policy that generated the action
        self.batch_log_policy = np.zeros([N, A, *self.policy_shape], dtype=np.float32)
        # log_policy for each role
        self.batch_role_log_policies = np.zeros([N, A, R, *self.policy_shape], dtype=np.float32)
        # float, batch_terminals[t] indicates if timestep t+1 was terminal or not
        self.batch_terminals = np.zeros([N, A], dtype=np.float32)
        # the current terminal state of each agent
        self.terminals = np.zeros([A], dtype=np.float32)
        # rewards generated at transition
        self.batch_ext_rewards = np.zeros([N, A], dtype=np.float32)
        self.batch_int_rewards = np.zeros([N, A], dtype=np.float32)
        # value estimates by agent just before action was taken
        self.batch_ext_value = np.zeros([N, A], dtype=np.float32)
        self.batch_int_value = np.zeros([N, A], dtype=np.float32)
        # return estimates (from which state though?)
        self.batch_ext_returns = np.zeros([N, A], dtype=np.float32)
        self.batch_int_returns = np.zeros([N, A], dtype=np.float32)
        # these are just for debugging
        self.batch_id = np.zeros([N, A], dtype=np.int64)
        self.batch_t = np.zeros([N, A], dtype=np.int64)

        # role of each player at each timestep
        self.batch_roles = np.zeros([N, A], dtype=np.int64)

        # these will be set if deception model is enabled
        self.batch_player_obs = None
        self.batch_player_obs_predictions = None
        self.batch_player_action_predictions = None
        self.batch_player_role_predictions = None

        # the true role of each player in the game that agent a is playing
        self.batch_player_roles = np.zeros([N, A, vec_env.max_players], dtype=np.int64)

        if self.uses_deception_model or self.uses_gv_model:
            # all player observations at time t for given game recorded for all agents
            # required for deception and gv model.
            self.batch_player_obs = np.zeros([N, A, vec_env.max_players, *self.obs_shape], dtype=np.uint8)

        # ground truth observations for each player
        # this is so when we take a random sample we always have the information we need.
        # It's a bit wasteful with memory though, especially with high player counts.
        if self.uses_deception_model:
            # unmodulated deception bonus for all players at each timestep.
            self.batch_raw_deception_bonus = np.zeros([N, A], dtype=np.float32)
            # policies for each player for each role at time t for given game recorded for all agents
            self.batch_player_role_policy = np.zeros([N, A, vec_env.max_players, R, *self.policy_shape], dtype=np.float32)
            # all policies at time t for given game recorded for all agents
            self.batch_player_policy = np.zeros([N, A, vec_env.max_players, *self.policy_shape], dtype=np.float32)
            # all terminals at time t for given game recorded for all agents
            self.batch_player_terminals = np.zeros([N, A, vec_env.max_players], dtype=np.float32)
            # which players this player can see
            self.batch_player_visible = np.zeros([N, A, vec_env.max_players], dtype=np.bool)

            if self.predicts_observations:
                # all player predictions at time t for given game recorded for all agents
                self.batch_player_obs_predictions = np.zeros([N, A, vec_env.max_players, *self.obs_shape],
                                                             dtype=np.uint8)

            if self.predicts_actions:
                self.batch_player_action_predictions = np.zeros([N, A, vec_env.max_players, R, *self.policy_shape],
                                                                dtype=np.float32)

            # each players prediction about the roles of all other players, as a log prob.
            self.batch_player_role_predictions = np.zeros([N, A, vec_env.max_players, R], dtype=np.float32)

            self.replay_buffer = []

        # advantage used during policy gradient (combination of intrinsic and extrinsic reward)
        self.batch_advantage = np.zeros([N, A], dtype=np.float32)

        self.ext_final_value_estimate = np.zeros([A], dtype=np.float32)
        self.int_final_value_estimate = np.zeros([A], dtype=np.float32)

        self.ems_norm = 0
        self.intrinsic_returns_rms = utils.RunningMeanStd(shape=())

        if utils.try_cast_to_int(micro_batch_size) is not None:
            self.micro_batch_size = utils.try_cast_to_int(micro_batch_size)
        elif micro_batch_size.lower() == "auto":
            self.micro_batch_size = self._get_auto_micro_batch_size()
        elif micro_batch_size.lower() == "off":
            self.micro_batch_size = float('inf')
        else:
            raise ValueError(f"Invalid micro batch size {micro_batch_size}")

        self.scaler = GradScaler() if self.amp else None

    def write_to_tensorboard(self, path):
        writer = SummaryWriter(path)
        blank_data = torch.zeros([32, 64, *self.obs_shape], dtype=torch.uint8).to(self.policy_model.device)
        writer.add_graph(self.policy_model, blank_data)
        #if self.uses_deception_model:
        #    writer.add_graph(self.deception_model, blank_data)
        writer.close()


    @property
    def mini_batches(self):
        return self.batch_size // self.mini_batch_size

    @property
    def uses_deception_model(self):
        return self.prediction_mode != self.DM_OFF

    @property
    def predicts_actions(self):
        return self.prediction_mode in [self.DM_ACTION, self.DM_BOTH]

    @property
    def predicts_observations(self):
        return self.prediction_mode in [self.DM_OBSERVATION, self.DM_BOTH]

    def save_logs(self, base_path):
        with open(os.path.join(base_path, "checkpoint_log.dat"), "wb") as f:
            pickle.dump(self.log, f)

        self.log.export_to_csv(os.path.join(base_path, "training_log.csv"))

    def save(self, filename):

        data = {
            'step': self.t,
            'model_state_dict': self.policy_model.state_dict(),
            'optimizer_state_dict': self.policy_optimizer.state_dict()
        }

        if self.uses_deception_model:
            data['deception_model_state_dict'] = self.deception_model.state_dict()
            data['deception_optimizer_state_dict'] = self.deception_optimizer.state_dict()

        if self.normalize_intrinsic_rewards:
            data['ems_norm'] = self.ems_norm
            data['intrinsic_returns_rms'] = self.intrinsic_returns_rms,

        torch.save(data, filename)

    def _eval(self):
        self.policy_model.eval()
        if self.deception_model is not None:
            self.deception_model.eval()

    def _train(self):
        self.policy_model.train()
        if self.deception_model is not None:
            self.deception_model.train()

    def _export_rollout(self, filename):
        """
        Write rollout to disk
        """
        # compression level 1 gives a lot of compression for our data, and is very fast.
        # level 9 is only 10% better, but 20x slower.
        with gzip.open(filename, "wb", compresslevel=1) as f:
            pickle.dump(self.batch_prev_obs, f)

    @profiler.record_function("learn")
    def learn(self, total_timesteps, reset_num_timesteps=True):

        if reset_num_timesteps:
            self.t = 0
            self.reset()
            self.learn_steps = 0

        batches = total_timesteps // self.batch_size
        for batch in range(batches):

            self.log.watch("epoch", self.t/1e6, display_width=8, display_priority=3, display_precision=1)

            start_time = time.time()
            self._eval()
            self.generate_rollout()
            self._train()
            self.train_policy()
            if self.uses_gv_model:
                self.train_global_value()
            if self.uses_deception_model:
                self.train_deception()

            end_time = time.time()
            step_time = end_time - start_time
            # measure number of agent interactions with environment per second
            self.log.watch_mean("ips", self.batch_size / step_time, history_length=4, display_width=8,
                                display_priority=2, display_precision=0)

            if self.export_rollout:
                self._export_rollout(os.path.join(self.log_folder, f"rollout_{self.learn_steps}.dat"))

            if self.verbose:
                if self.learn_steps % 10 == 0:
                    print(f"{self.learn_steps}, {self.t / 1000 / 1000:.1f}M")
                self.log.print_variables(include_header=self.learn_steps % 10 == 0)

            self.t += self.batch_size
            self.learn_steps += 1

            self.log.record_step()

    def _get_auto_micro_batch_size(self):
        """ Returns an appropriate micro-batch size. """

        # working this out will be complex
        # maybe can assume that 12GB gives us 4096 with AMP and 2048 without
        if self.policy_model.encoder_type is DefaultEncoder:
            micro_batch_size = 4096
        elif self.policy_model.encoder_type is FastEncoder:
            micro_batch_size = 8192
        else:
            micro_batch_size = 1024 # just a guess

        if self.uses_deception_model:
            micro_batch_size //= 2

        if self.amp:
            # amp seems to work very poorly for large batchsizes (perhaps due to the issue with adding lots of small
            # gradients together. If we keep the micro_batch_size small it works ok, but performance is not great.
            micro_batch_size = min(1024, micro_batch_size)
        if self.data_parallel:
            micro_batch_size *= 4 # should be number of GPUs

        return micro_batch_size

    def reset(self):
        # initialize agent
        self.agent_obs = self.vec_env.reset()
        self.agent_rnn_state *= 0
        self.episode_score *= 0
        self.episode_len *= 0
        self.terminals *= 0

    def calculate_deception_bonus_from_observations(
            self,
            player_observation_prediction_predictions: torch.Tensor,
            prior_role_belief: torch.Tensor,
            believed_policy_rnn_states: torch.Tensor,
            terminals: torch.Tensor,
            true_role: torch.Tensor,
            mask: torch.Tensor,
            actions: torch.Tensor,
        ):
        """
        Calculates a deception bonus based on modifiying other agents belief under the assumption that they are
        baysian agents, and using our own estimations of their estimations of our likely actions for each role.

        :param player_observation_prediction_predictions: Sequence of observations that represent agent i's belief about agent j's
            belief about i's observations.
            Tensor of dims [B, n_players, *obs_space] of type uint8
        :param prior_role_belief: Agent i's belief about agent j's belief about agent i's true role. These should be log probabilities
            Tensor of dims [B, n_players, n_roles] of type float32

        :param believed_policy_rnn_states: the players current estimate of the policy rnn states for the other players
            Tensor of dims [B, n_players, *rnn_policy_state_space] of type float32

        :param terminals: The true terminals for each player, of shape [B, n_players] of type bool
        :param true_role: The true roles for each player, of shape [B] of type long
        :param mask: A mask indicating which players we are trying to deceive, and therefore get a bonus for.
            This can be used to mask out invisible players
            Tensor of dims [B, n_players] of type bool

        :param actions: Actions for each agent
            Tensor of dims [B] of type long

        :return: A tuple containing
            The bonus for each player summed over other players, tensor of dims [B],
            The updated rnn_states, tensor of dims [B, n_players, *rnn_policy_state_space
        """

        assert player_observation_prediction_predictions.dtype == torch.uint8

        B, n_players, *obs_shape = player_observation_prediction_predictions.shape

        # upload rnn_states if needed
        believed_policy_rnn_states = believed_policy_rnn_states.to(device=self.policy_model.device)

        # first get policy output for predicted observations, the output will include policy for each role
        with torch.no_grad():
            pred_policy_out, new_rnn_states = self.policy_model.forward_sequence(
                player_observation_prediction_predictions.reshape(1, B * n_players, *self.obs_shape),
                rnn_states=believed_policy_rnn_states.reshape((B * n_players, 2, self.policy_memory_units)),
                terminals=terminals.reshape(1, B * n_players)
            )

        # this will be [1, B*n_players, n_roles, n_actions], so drop first dim, and reshape
        _, _, n_roles, n_actions = pred_policy_out["role_log_policy"].shape
        role_log_policy = pred_policy_out["role_log_policy"].reshape(B, n_players, n_roles, n_actions)
        # states need to be reshaped too
        new_rnn_states = new_rnn_states.reshape(B, n_players, 2, self.policy_memory_units)

        return calculate_deception_bonus_from_actions(
            player_action_prediction_predictions=role_log_policy,
            prior_role_belief=prior_role_belief,
            true_role=true_role,
            mask=mask,
            actions=actions,
            device=self.deception_model.device
        ), new_rnn_states

    def _duplicate_players(self, x, n_players):
        """ Returns a copy of data with players duplicated.
            If input is

            [A1, B1, C1, A2, B2, C2]

            output will be

            [[A1, B1, C1], [A1, B1, C1], [A1, B1, C1],
             [A2, B2, C2], [A2, B2, C2], [A2, B2, C2]]

            This means that each row contains all the required information about each other player in their game.

            x: [n_games*n_players, *data_shape]
            returns [n_games*n_players, n_players, *data_shape]
        """
        data_shape = x.shape[1:]
        assert len(x) % n_players == 0
        x = x.reshape(len(x)//n_players, n_players, *data_shape)
        x = x.repeat(n_players, axis=0)
        return x


    @profiler.record_function("calculate_deception")
    def calculate_deception_bonus(self,
                                  model_out: dict,
                                  actions: np.ndarray,
                                  env: MultiAgentVecEnv,
                                  roles: np.ndarray,
                                  is_visible: np.ndarray,
                                  believed_other_rnn_states=None,
                                  prev_terminals=None
                                  ):
        """
        Calculate (unmodulated) deception bonus for all players
        :param model_out:
        :param actions:
        :param env:
        :param believed_other_rnn_states:
        :param prev_terminals:
        :return: np array of dims [B] containing deception bonus for each agent
        """

        B = len(actions) # number of agents in batch.
        n_players = env.max_players
        deception_bonus = np.zeros([B], dtype=np.float)

        # generate a mask that assigns bonus only for deceiving enemy players (or players thought to be
        # enemies.

        bonus_mask = np.ones((B, n_players), dtype=np.float32)

        # filter out players who are not visible, as they can not observe the action
        bonus_mask[:, :] = is_visible

        # we don't try and deceive ourselves
        # this isn't needed as agents will quickly set their prior to 1 for themselves, effectively masking out the
        # deception.
        # indexes = np.asarray([player.index for player in env.players])
        #bonus_mask[range(len(bonus_mask)), indexes] = 0

        # also: don't give bonus to dead players, and don't factor in dead players beliefs
        is_dead = np.asarray([player.is_dead for player in env.players])
        bonus_mask[is_dead] = 0
        bonus_mask[self._duplicate_players(is_dead, n_players)] = 0

        # filter out players who could not have seen the action

        bonus_mask = torch.from_numpy(bonus_mask)

        # there are two ways of doing this, the predict actions method and the predict observations method
        if self.predicts_actions:
            db_actions = calculate_deception_bonus_from_actions(
                player_action_prediction_predictions=model_out["action_backwards_prediction"],
                prior_role_belief=model_out["role_backwards_prediction"],
                true_role=torch.from_numpy(roles),
                mask=bonus_mask,
                actions=torch.from_numpy(actions),
                device=self.deception_model.device
            )
            deception_bonus += db_actions.cpu().detach().numpy()

        if self.predicts_observations:
            expanded_terminals = torch.from_numpy(
                self._duplicate_players(prev_terminals, n_players)).to(device=self.policy_model.device)
            obs_backwards_predictions = image_to_uint8(model_out["obs_backwards_prediction"])
            db_observations, new_believed_other_rnn_states = self.calculate_deception_bonus_from_observations(
                player_observation_prediction_predictions=obs_backwards_predictions,
                prior_role_belief=model_out["role_backwards_prediction"],
                believed_policy_rnn_states=believed_other_rnn_states,
                terminals=expanded_terminals,
                true_role=torch.from_numpy(roles),
                mask=bonus_mask,  # for the moment don't mask non-visible players
                actions=torch.from_numpy(actions)
            )
            believed_other_rnn_states[:] = new_believed_other_rnn_states # copy across new state
            deception_bonus += db_observations.cpu().detach().numpy()

        return deception_bonus

    @profiler.record_function("generate_rollout")
    def generate_rollout(self):
        """
        Runs agents through environment for n_steps steps, recording the transitions as it goes.
        :return: Nothing
        """

        def swap_prediction_targets(x: Union[torch.Tensor, np.ndarray]):
            """
            Takes a tensor [n_games * n_players, n_players, ...] containing rows of the ith players predictions
            and transposes so that each row now contains other players predictions of the ith player.
            :param x: Tensor of dims [n_games * n_players, n_players, ...]
            :return: Tensor of dims [n_games * n_players, n_players, ...]
            """

            b, n_players, *data_shape = x.shape
            assert b % n_players == 0
            n_games = b // n_players

            x = x.reshape(n_games, n_players, n_players, *data_shape)
            if type(x) is torch.Tensor:
                # torch v1.8 has swap axes, but I'm on 1.7.1
                # np.transpose != torch.transpose, so we really need to use swap axes.
                x = x.transpose(1, 2)
            else:
                x = x.swapaxes(1, 2)
            x = x.reshape(n_games * n_players, n_players, *data_shape)
            return x

        n_roles = self.vec_env.max_roles
        n_players = self.vec_env.max_players
        n_games = len(self.vec_env.games)
        B = n_games * n_players

        assert B == len(self.vec_env.players)

        believed_other_rnn_states = torch.zeros(
            [self.n_agents, n_players, 2, self.policy_memory_units], dtype=torch.float32
        )

        # these should not change over time
        ids = np.asarray([player.index for player in self.vec_env.players])

        with torch.no_grad():

            for t in range(self.n_steps):

                self.batch_rnn_states[t, :] = self.agent_rnn_state.cpu()[:]

                # get time stamps from environment
                ts = []
                for game in self.vec_env.games:
                    for player in game.players:
                        ts.append(game.round_timer)
                ts = np.asarray(ts)


                # ------------------------------------
                # roles
                roles = self.vec_env.get_roles()
                self.batch_roles[t] = roles

                prev_obs = self.agent_obs.copy() #[A, *obs_shape]
                player_obs = self._duplicate_players(prev_obs, n_players)
                prev_terminals = self.terminals.copy()

                # forward state through model, then detach the result and convert to numpy.
                with profiler.record_function("gr_model_step"):
                    model_out, new_agent_rnn_state = self.forward(
                        torch.from_numpy(self.agent_obs),
                        self.agent_rnn_state,
                        roles=torch.from_numpy(roles),
                        global_obs=torch.from_numpy(player_obs)
                    )
                self.agent_rnn_state[:] = new_agent_rnn_state # we copy into the states to avoid allocating another buffer.

                # map across values if we are using global value
                if self.uses_gv_model:
                    assert "global_ext_value" in model_out, \
                        "Global value estimates missing from model output, did you forget to pass global_observations during forward?"
                    # global values come out as [B, n_players], so we need to pick out the players needed.
                    for value in ['ext', 'int']:
                        model_out[f"{value}_value"] = model_out[f"global_{value}_value"][range(B), ids]

                role_log_policies = model_out["role_log_policy"].detach().cpu().numpy()
                log_policy = model_out["log_policy"].detach().cpu().numpy()
                ext_value = model_out["ext_value"].detach().cpu().numpy()

                # save who is which role
                player_roles = self._duplicate_players(roles, n_players)  # batch_roles is [A, n_players]
                self.batch_player_roles[t] = player_roles

                if self.uses_deception_model:

                    # calculate who is visible [B, n_players]
                    players_visible = []
                    for game in self.vec_env.games:
                        for player in game.players:
                            vision = [player.in_vision(other_player.x, other_player.y) for other_player in game.players]
                            players_visible.append(vision)
                    players_visible = np.asarray(players_visible)

                    # role prediction
                    self.batch_player_policy[t] = self._duplicate_players(log_policy, n_players)
                    self.batch_player_role_policy[t] = self._duplicate_players(role_log_policies, n_players)
                    self.batch_player_terminals[t] = self._duplicate_players(prev_terminals, n_players)
                    self.batch_player_visible[t] = players_visible

                    if self.predicts_observations:
                        # predictions of other players observations
                        self.batch_player_obs[t] = player_obs
                        # organise prediction predictions
                        player_obs_predictions = image_to_uint8(model_out["obs_prediction"].detach().cpu())
                        self.batch_player_obs_predictions[t] = swap_prediction_targets(player_obs_predictions)

                    if self.predicts_actions:
                        # again for actions...
                        player_action_predictions = model_out["action_prediction"].cpu().detach()
                        self.batch_player_action_predictions[t] = swap_prediction_targets(player_action_predictions)

                if self.batch_player_role_predictions is not None:
                    # record role predictions
                    # role_predictions come out as n_games*n_players, n_players, n_roles,
                    # we reshape it to [n_games, n_players, n_player, n_roles] then swap the n_players axis
                    # if we index in as [n, i, j, r] then we are making it so that the row [n, :, ...] records
                    # all other players predictions of this players role. (which are the targets we need)
                    # record role predictions
                    player_role_predictions = model_out["role_prediction"].detach().cpu().numpy()
                    self.batch_player_role_predictions[t] = swap_prediction_targets(player_role_predictions)

                # ---------------------------------------------------------------
                # sample actions and run through environment.
                actions = np.asarray([utils.sample_action_from_logp(prob) for prob in log_policy], dtype=np.int32)
                with profiler.record_function("gr_env_step"):
                    self.agent_obs, ext_rewards, new_terminals, infos = self.vec_env.step(actions)
                # ---------------------------------------------------------------

                self.terminals[:] = new_terminals

                # work out our intrinsic rewards
                int_value = model_out["int_value"].detach().cpu().numpy()
                int_rewards = np.zeros_like(ext_rewards)

                if self.uses_deception_model:
                    raw_deception_bonus = self.calculate_deception_bonus(
                        model_out, actions, self.vec_env, roles, players_visible,
                        believed_other_rnn_states if self.predicts_observations else None,
                        prev_terminals if self.predicts_observations else None
                    )

                    self.batch_raw_deception_bonus[t] = raw_deception_bonus

                    # modulate deception bonus
                    # note we multiply by max(deception_bonus) later on, so normalize it to 1 here
                    if self.deception_bonus is float:
                        deception_bonus = raw_deception_bonus
                    else:
                        if max(self.deception_bonus) == 0:
                            deception_bonus = raw_deception_bonus * 0
                        else:
                            deception_bonus = raw_deception_bonus * \
                                np.asarray([self.deception_bonus[r] for r in roles]) / max(self.deception_bonus)

                    # turn off deception bonus until we get through the warm up peroid, then slowly introduce it
                    if self.t < self.deception_bonus_start_timestep:
                        deception_bonus *= 0
                    elif self.deception_bonus_start_timestep < self.t < self.deception_bonus_start_timestep * 2:
                        factor = (self.t - self.deception_bonus_start_timestep) / self.deception_bonus_start_timestep
                        deception_bonus *= factor

                    # make sure bonus is reasonable by setting nan to 0 and applying clipping
                    if np.any(np.isnan(deception_bonus)):
                        self.log.warn("Nan found in deception bonus, setting to 0.")
                        deception_bonus[np.isnan(deception_bonus)] = 0
                    deception_bonus = np.clip(deception_bonus, -10, 10)


                    int_rewards += deception_bonus

                # save raw rewards for monitoring the agents progress
                raw_rewards = np.asarray([info.get("raw_reward", reward) for reward, info in zip(ext_rewards, infos)],
                                         dtype=np.float32)

                self.episode_score += raw_rewards
                self.episode_len += 1

                for i, terminal in enumerate(self.terminals):
                    if terminal:
                        # reset the internal state on episode completion
                        self.agent_rnn_state[i] *= 0
                        # reset is handled automatically by vectorized environments
                        # so just need to keep track of book-keeping
                        self.log.watch_full("ep_score", self.episode_score[i], display_width=0)
                        self.log.watch_full("ep_length", self.episode_len[i],  display_width=0)
                        self.log.watch_mean("mean_ep_score", self.episode_score[i], display_width=8, display_precision=2)
                        self.log.watch_mean("mean_ep_length", self.episode_len[i], display_width=8)
                        self.episode_score[i] = 0
                        self.episode_len[i] = 0

                with profiler.record_function("gr_copy"):
                    self.batch_prev_obs[t] = prev_obs
                    self.batch_actions[t] = actions
                    self.batch_ext_rewards[t] = ext_rewards
                    self.batch_log_policy[t] = log_policy
                    self.batch_role_log_policies[t] = role_log_policies
                    self.batch_terminals[t] = prev_terminals
                    self.batch_int_rewards[t] = int_rewards
                    self.batch_ext_value[t] = ext_value
                    self.batch_int_value[t] = int_value
                    self.batch_id[t] = ids
                    self.batch_t[t] = ts

            # get value estimates for final observation.
            model_out, _ = self.forward(
                torch.from_numpy(self.agent_obs),
                self.agent_rnn_state,
                roles=torch.from_numpy(roles),
                global_obs=torch.from_numpy(self._duplicate_players(self.agent_obs, n_players)) if self.uses_gv_model else None
            )

            self.ext_final_value_estimate = model_out["ext_value"].detach().cpu().numpy()
            if "int_value" in model_out:
                self.int_final_value_estimate = model_out["int_value"].detach().cpu().numpy()

        self.calculate_returns()

    def watch_per_team(self, var_name, values, reduction=np.mean, **kwargs):
        """
        Logs a variable with per team values
        :param var_name:
        :param values: np.array of dims [N, B]
        :param reduction:
        :param kwargs:
        :return:
        """

        assert values.shape == self.batch_roles.shape

        # log the variable
        self.log.watch_mean(var_name, reduction(values), **kwargs)
        kwargs["display_width"] = 0
        for team_id, team_name in ((0, 'red'), (1, 'green'), (2, 'blue')):
            mask = self.batch_roles == team_id
            data = values[mask]
            if len(data) > 0:
                self.log.watch_mean(f"{team_name}_{var_name}", reduction(data), **kwargs)

    def calculate_returns(self):
        """
        Calculate returns from previous rollout.
        Updates the following variables
            ext_returns, int_returns
            ext_advantage
        :return:
        """

        ext_advantage = calculate_gae(self.batch_ext_rewards, self.batch_ext_value, self.ext_final_value_estimate,
            self.batch_terminals, self.terminals, self.gamma, self.gae_lambda)
        self.batch_ext_returns = ext_advantage + self.batch_ext_value

        # calculate the intrinsic returns
        if self.normalize_intrinsic_rewards:
            # normalize returns using EMS
            for t in range(self.n_steps):
                self.ems_norm = 0.99 * self.ems_norm + self.batch_int_rewards[t, :]
                self.intrinsic_returns_rms.update(self.ems_norm.reshape(-1))
            # normalize the intrinsic rewards
            self.intrinsic_reward_norm_scale = (1e-5 + self.intrinsic_returns_rms.var ** 0.5)
            scaled_int_rewards = self.batch_int_rewards / self.intrinsic_reward_norm_scale
        else:
            self.intrinsic_reward_norm_scale = 1
            scaled_int_rewards = self.batch_int_rewards

        # optionaly let returns propagate through terminal states.
        int_advantage = calculate_gae(scaled_int_rewards, self.batch_int_value, self.int_final_value_estimate,
                                           self.batch_terminals * (not self.intrinsic_reward_propagation),
                                           self.terminals * (not self.intrinsic_reward_propagation),
                                           self.gamma_int, self.gae_lambda)
        self.batch_int_returns = int_advantage + self.batch_int_value

        # sum them together
        self.batch_advantage = self.extrinsic_reward_scale * ext_advantage + self.intrinsic_reward_scale * int_advantage
        if self.normalize_advantages:
            self.batch_advantage = (self.batch_advantage - self.batch_advantage.mean()) / (self.batch_advantage.std() + 1e-8)

        self.log.watch_mean("adv_mean", np.mean(self.batch_advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_std", np.std(self.batch_advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_max", np.max(self.batch_advantage), display_width=0 if self.normalize_advantages else 0)
        self.log.watch_mean("adv_min", np.min(self.batch_advantage), display_width=0 if self.normalize_advantages else 0)
        self.watch_per_team("batch_reward_ext", self.batch_ext_rewards, display_name="rew_ext", display_width=0)
        self.watch_per_team("batch_return_ext", self.batch_ext_returns, display_name="ret_ext")
        self.log.watch_mean("batch_return_ext_std", np.std(self.batch_ext_returns), display_name="ret_ext_std",
                            display_width=0)
        self.watch_per_team("value_est_ext", self.batch_ext_value, display_name="est_v_ext")
        self.log.watch_mean("value_est_ext_std", np.std(self.batch_ext_value), display_name="est_v_ext_std", display_width=0)
        self.log.watch_mean("ev_ext", utils.explained_variance(self.batch_ext_value.ravel(), self.batch_ext_returns.ravel()))

        if self.uses_deception_model:

            # log intrinsic rewards per team
            self.watch_per_team("raw_deception_bonus", self.batch_raw_deception_bonus, display_width=0)
            self.watch_per_team("batch_reward_int", self.batch_int_rewards, display_name="rew_int", display_width=0)
            self.log.watch_mean("batch_reward_int_std", np.std(self.batch_int_rewards), display_name="rew_int_std", display_width=0)
            self.watch_per_team("batch_return_int", self.batch_int_returns, display_name="ret_int")
            self.log.watch_mean("batch_return_int_std", np.std(self.batch_int_returns), display_name="ret_int_std")

            self.watch_per_team("value_est_int", self.batch_int_value, display_name="est_v_int")
            self.log.watch_mean("value_est_int_std", np.std(self.batch_int_value), display_name="est_v_int_std")
            self.log.watch_mean("ev_int", utils.explained_variance(self.batch_int_value.ravel(), self.batch_int_returns.ravel()))

            if self.normalize_intrinsic_rewards:
                self.log.watch_mean("norm_scale_int", self.intrinsic_reward_norm_scale, display_width=12)

    def extract_policy_rnn_states(self, rnn_states):
        # input can be [..., 2, memory_units]
        return rnn_states[..., :self.policy_memory_units]

    def extract_deception_rnn_states(self, rnn_states):
        # input can be [..., 2, memory_units]
        assert self.uses_deception_model
        return rnn_states[..., self.policy_memory_units: self.policy_memory_units + self.dm_memory_units]

    def extract_gv_rnn_states(self, rnn_states):
        # input can be [..., 2, memory_units]
        assert self.uses_gv_model
        if self.uses_deception_model:
            return rnn_states[..., self.policy_memory_units + self.dm_memory_units:]
        else:
            return rnn_states[..., self.policy_memory_units:]

    @property
    def uses_gv_model(self):
        return self.gv_model is not None

    def compact_rnn_states(self, policy_states, deception_states=None, gv_states=None):
        """
        Compacts the rnn states for all 3 modules into one state.
        :param policy_states:
        :param deception_states:
        :param gv_states:
        :return:
        """
        if self.uses_gv_model and not self.uses_deception_model:
            return torch.cat([policy_states, gv_states], dim=-1)
        if self.uses_deception_model and not self.uses_gv_model:
            return torch.cat([policy_states, deception_states], dim=-1)
        if self.uses_deception_model and self.uses_gv_model:
            return torch.cat([policy_states, deception_states, gv_states], dim=-1)
        return policy_states

    @profiler.record_function("gv_forward")
    def forward_gv_mini_batch(self, data, loss_scale=1):

        loss: torch.Tensor = torch.tensor(0, dtype=torch.float32, device=self.policy_model.device, requires_grad=True)

        prev_obs = data["player_obs"]
        terminals = data["terminals"]
        rnn_states = data["rnn_states"][0]  # get states at start of window
        ids = merge_down(data["id"])

        # these were provided in N, B format but we want them just as one large batch.
        weights = 1

        with profiler.record_function("gv_forward_model"):
            model_out, _ = self.gv_model.forward_sequence(
                obs=prev_obs,
                rnn_states=rnn_states,
                terminals=terminals
            )

        # output will be a sequence of [N, B] but we want this just as [N*B] for processing
        for k, v in model_out.items():
            model_out[k] = merge_down(v)

        # -------------------------------------------------------------------------
        # Calculate loss_value
        # -------------------------------------------------------------------------

        value_heads = ["ext", "int"]


        for value_head in value_heads:

            # value predictions will come out as [N * B, n_players], so we need to index in to find the correct ones
            value_prediction = model_out["global_{}_value".format(value_head)][range(len(ids)), ids]
            returns = merge_down(data["{}_returns".format(value_head)])
            old_pred_values = merge_down(data["{}_value".format(value_head)])

            if self.use_clipped_value_loss:
                # is is essentially trust region for value learning, and seems to help a lot.
                value_prediction_clipped = old_pred_values + torch.clamp(value_prediction - old_pred_values,
                                                                         -self.ppo_epsilon, +self.ppo_epsilon)
                vf_losses1 = (value_prediction - returns).pow(2)
                vf_losses2 = (value_prediction_clipped - returns).pow(2)
                loss_value = self.vf_coef * 0.5 * torch.mean(torch.max(vf_losses1, vf_losses2) * weights)
            else:
                # simpler version, just use MSE.
                vf_losses1 = (value_prediction - returns).pow(2)
                loss_value = self.vf_coef * 0.5 * torch.mean(vf_losses1 * weights)

            self.log.watch_mean("gvl_v_" + value_head, loss_value, history_length=64)
            loss = loss + loss_value

        # -------------------------------------------------------------------------
        # Apply loss
        # -------------------------------------------------------------------------

        loss = loss * loss_scale
        self.log.watch_mean("gv_loss", loss)

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(-loss).backward()
            self.log.watch_mean("s_unskip", self.scaler._get_growth_tracker())
            self.log.watch_mean("s_scale", self.scaler.get_scale())
        else:
            loss.backward()

        self.gv_batch_counter += 1

    @profiler.record_function("algorithm_forward")
    def forward(
            self,
            obs: torch.Tensor,
            rnn_states:torch.Tensor,
            roles: torch.Tensor,
            global_obs: torch.Tensor = None,
            disable_deception=False
    ):
        """
        Forwards a single observations through model for each agent
        For more advanced uses call model.forward_sequence directly.

        :param obs: Observations for each agent, tensor of dims [B, *observation_shape]
        :param rnn_states: tensor of dims [B, 2|4, memory_units]
        :param global_obs: observations for all players in each agents game [B, n_players, *observation_shape] (optional)
        :param roles: role for each player, [B]

        :return: a tuple(
            dictionary containing,
                'log_policy',       [B, actions]
                'ext_value', and    [B]
                'int_value'.        [B],
            new_rnn_states          [B, 2|4, memory_dims]

        """

        # check the input
        (B, *obs_shape), device = obs.shape, obs.device
        check_tensor("obs", obs, (B, *obs_shape), torch.uint8)
        check_tensor("rnn_states", rnn_states, (B, *self.rnn_state_shape), torch.float)
        check_tensor("roles", roles, (B,), torch.int64)

        # handle very large forwards by splitting batch into smaller parts
        batch_size = obs.shape[0]
        max_mini_batch_size = 8192
        if batch_size > max_mini_batch_size:
            out_parts, state_parts = [], []
            for i in range(math.ceil(batch_size/max_mini_batch_size)):
                portion = list(range(i*max_mini_batch_size, min((i+1) * max_mini_batch_size, batch_size)))
                out, state = self.forward(obs[portion], rnn_states[portion])
                out_parts.append(out)
                state_parts.append(state)
            # put them together
            rnn_states = torch.cat(state_parts, dim=0)
            output = {}
            for k in out_parts[0].keys():
                output[k] = torch.cat([out[k] for out in out_parts], dim=0)
            return output, rnn_states

        new_policy_states = None
        new_gv_states = None
        new_deception_states = None

        # make a sequence of length 1 and forward it through model
        result, new_policy_states = self.policy_model.forward_sequence(
            obs=obs[np.newaxis],
            rnn_states=self.extract_policy_rnn_states(rnn_states),
            roles=roles[np.newaxis, :]
        )

        # global value (requires special input, otherwise is ignored)
        if self.gv_model:
            if global_obs is not None:
                gv_results, new_gv_states = self.gv_model.forward_sequence(
                    obs=global_obs[np.newaxis],
                    rnn_states=self.extract_gv_rnn_states(rnn_states)
                )
                for k, v in gv_results.items():
                    result[k] = v
            else:
                # pass through rnn states
                new_gv_states = self.extract_gv_rnn_states(rnn_states)

        # deception module
        if self.uses_deception_model:
            if disable_deception:
                # just copy old states through
                new_deception_states = self.extract_deception_rnn_states(rnn_states)
            else:
                deception_results, new_deception_states = self.deception_model.forward_sequence(
                    obs=obs[np.newaxis],
                    rnn_states=self.extract_deception_rnn_states(rnn_states)
                )
                for k, v in deception_results.items():
                    result[k] = v

        new_rnn_states = self.compact_rnn_states(new_policy_states, new_deception_states, new_gv_states)

        # remove the sequence of length 1
        unsqueze_result = {}
        for k, v in result.items():
            unsqueze_result[k] = v[0]

        return unsqueze_result, new_rnn_states

    @profiler.record_function("minibatch_forward")
    def forward_policy_mini_batch(self, data, loss_scale=1):
        """
        Run mini batch through model generating loss, call opt_step_mini_batch to apply the update.
        mini batch should have the structure [N, B, *] where N is the sequence length, and is the batch length

        :param data: dictionary containing data for
        """

        loss: torch.Tensor = torch.tensor(0, dtype=torch.float32, device=self.policy_model.device, requires_grad=True)

        # -------------------------------------------------------------------------
        # Calculate loss_pg
        # -------------------------------------------------------------------------

        roles = data["roles"]
        prev_obs = data["prev_obs"]
        terminals = data["terminals"]
        rnn_states = data["rnn_states"][0] # get states at start of window

        # these were provided in N, B format but we want them just as one large batch.
        actions = merge_down(data["actions"])
        policy_log_probs = merge_down(data["log_policy"])
        advantages = merge_down(data["advantages"])
        weights = 1

        N, B, *obs_shape = prev_obs.shape

        with profiler.record_function("fmb_forward_model"):
            model_out, _ = self.policy_model.forward_sequence(
                obs=prev_obs,
                rnn_states=rnn_states,
                terminals=terminals,
                roles=roles
            )

        # output will be a sequence of [N, B] but we want this just as [N*B] for processing
        for k, v in model_out.items():
            if k != "policy_role_prediction":
                model_out[k] = merge_down(v)

        logps = model_out["log_policy"]

        actions = actions.to(dtype=torch.int64) # required for indexing

        ratio = torch.exp(logps[range(N*B), actions] - policy_log_probs[range(N*B), actions])
        clipped_ratio = torch.clamp(ratio, 1 - self.ppo_epsilon, 1 + self.ppo_epsilon)

        loss_clip = torch.min(ratio * advantages, clipped_ratio * advantages)
        loss_clip_mean = (weights*loss_clip).mean()

        self.log.watch_mean("loss_pg", loss_clip_mean, history_length=64, display_precision=3)
        loss = loss + loss_clip_mean

        # ----------------------------------------
        # role prediction loss
        # ----------------------------------------
        # the policy module also makes predictions about who is who, so that the feature space contains this
        # information and can be used for voting etc
        # loss must be negative as we maximize loss with policy...

        if "policy_role_prediction" in model_out:
            loss_role = -0.1 * calculate_roll_prediction_nll(
                model_out["policy_role_prediction"],
                data["player_roles"]
            ).mean()
            loss = loss + loss_role
            self.log.watch_mean("loss_policy_role", loss_role)

            # get clean roll_loss for each team, this is used for debugging only
            if self.policy_batch_counter % 10 == 0:
                with torch.no_grad():
                    for team_id, team_name in ((0, 'red'), (1, 'green'), (2, 'blue')):
                        team_filter = data["roles"] == team_id
                        nll = calculate_roll_prediction_nll(model_out["policy_role_prediction"], data["player_roles"], team_filter)
                        if nll is not None:
                            self.log.watch_mean(f"{team_name}_policy_role_nll", nll.mean(), display_width=0)

        # -------------------------------------------------
        # calculate kl between policies, and record entropy
        # -------------------------------------------------
        # note: this is a little odd as the input will be role dependant, but it should still give us some idea
        # of how the policies diverge
        # also, this is a little slow so we do it only on occasion
        if self.policy_batch_counter % 10 == 9:
            for role_a in range(3):
                a = data["role_log_policy"][..., role_a, :]
                entropy = torch.sum(-torch.exp(a) * a, dim=-1).mean()
                self.log.watch_mean(f"entropy_{role_a}", entropy, display_width=0)
                for role_b in range(3):
                    b = data["role_log_policy"][..., role_b, :]
                    kl = F.kl_div(a, b, reduction='batchmean', log_target=True)
                    self.log.watch_mean(f"kl_{role_a}_{role_b}", kl, display_width=0)

        # -------------------------------------------------------------------------
        # Calculate loss_value
        # -------------------------------------------------------------------------

        value_heads = ["ext", "int"]

        for value_head in value_heads:
            value_prediction = model_out["{}_value".format(value_head)]
            returns = merge_down(data["{}_returns".format(value_head)])
            old_pred_values = merge_down(data["{}_value".format(value_head)])

            if self.use_clipped_value_loss:
                # is is essentially trust region for value learning, and seems to help a lot.
                value_prediction_clipped = old_pred_values + torch.clamp(value_prediction - old_pred_values,
                                                                         -self.ppo_epsilon, +self.ppo_epsilon)
                vf_losses1 = (value_prediction - returns).pow(2)
                vf_losses2 = (value_prediction_clipped - returns).pow(2)
                loss_value = -self.vf_coef * 0.5 * torch.mean(torch.max(vf_losses1, vf_losses2) * weights)
            else:
                # simpler version, just use MSE.
                vf_losses1 = (value_prediction - returns).pow(2)
                loss_value = -self.vf_coef * 0.5 * torch.mean(vf_losses1 * weights)

            self.log.watch_mean("loss_v_" + value_head, loss_value, history_length=64)
            loss = loss + loss_value

        # -------------------------------------------------------------------------
        # Calculate loss_entropy
        # -------------------------------------------------------------------------

        loss_entropy = -(logps.exp() * logps).sum(axis=1)
        loss_entropy = (loss_entropy * weights * self.entropy_bonus).mean()
        self.log.watch_mean("loss_ent", loss_entropy)
        loss = loss + loss_entropy

        # -------------------------------------------------------------------------
        # Apply loss
        # -------------------------------------------------------------------------

        loss = loss * loss_scale
        self.log.watch_mean("loss", loss)

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(-loss).backward()
            self.log.watch_mean("s_unskip", self.scaler._get_growth_tracker())
            self.log.watch_mean("s_scale", self.scaler.get_scale())
        else:
            (-loss).backward()

        self.policy_batch_counter += 1

    @profiler.record_function("deception_back")
    def forward_deception_mini_batch(self, data, loss_scale=1):

        def calculate_kl(obs_predictions):
            """
            Calculates KL between policy of true observation and policy of predicted observation
            # obs predictions are [N, B, n_players, *obs_shape] and should not be filtered
            :return:
            """

            N, B, n_players, *obs_shape = obs_predictions.shape

            true_roles = data["player_roles"].reshape(N, B * n_players) #[N, B, n_players]

            n_players = self.vec_env.max_players
            predicted_initial_policy_rnn_states = self.extract_policy_rnn_states(
                self.get_initial_rnn_state(B * n_players))
            # obs predictions are [N, B, n_players, *obs_shape], so map down
            pred_policy_out, _ = self.policy_model.forward_sequence(
                obs_predictions.reshape(N, B * n_players, *self.obs_shape),
                rnn_states=predicted_initial_policy_rnn_states,
                terminals=data["player_terminals"].reshape(N, B * n_players),
                roles=true_roles
            )

            # put back into [N, B, n_players, *] for processing, and remove first half of window
            # (this allows LSTM to warm up)
            true_obs_log_policy = data["player_policy"].reshape(N, B, n_players, *self.policy_shape)[N // 2:]
            pred_obs_log_policy = pred_policy_out['log_policy'].reshape(N, B, n_players, *self.policy_shape)[N // 2:]

            # filter out players that are not visible
            true = filter_visible(true_obs_log_policy, player_visible[N // 2:])
            pred = filter_visible(pred_obs_log_policy, player_visible[N // 2:])

            # check the KL between these two
            # kl is [N*B*n_players, *policy_shape]
            kl = torch.exp(true) * (true - pred)
            kl = kl.sum(dim=-1)
            kl = kl.mean()

            return kl

        loss = torch.tensor(0, dtype=torch.float32, device=self.deception_model.device)

        prev_obs = data["prev_obs"]
        terminals = data["terminals"]
        rnn_states = data["rnn_states"][0] # get states at start of window
        N, B, *obs_shape = prev_obs.shape
        n_players = self.vec_env.max_players
        n_roles = 3

        # calculate
        model_out, _ = self.deception_model.forward_sequence(
            obs=prev_obs,
            rnn_states=rnn_states,
            terminals=terminals
        )

        player_visible = data["player_visible"]
        self.log.watch_mean("visible",
                            torch.mean(player_visible.float()))  # record what percentage of pairs have vision
        # -------------------------------------------------------------------------
        # Calculate loss_role
        # -------------------------------------------------------------------------

        loss_role = self.dm_loss_scale * calculate_roll_prediction_nll(model_out["role_prediction"], data["player_roles"]).mean()
        loss += loss_role
        self.log.watch_mean("loss_role", loss_role)

        # get clean roll_loss for each team, this is used for debugging only
        if self.deception_batch_counter % 10 == 0:
            with torch.no_grad():
                for team_id, team_name in ((0, 'red'), (1, 'green'), (2, 'blue')):
                    team_filter = data["roles"] == team_id
                    nll = calculate_roll_prediction_nll(model_out["role_prediction"], data["player_roles"], team_filter)
                    if nll is not None:
                        self.log.watch_mean(f"{team_name}_role_nll", nll.mean(), display_width=0)

        if "role_backwards_prediction" in model_out:
            role_backwards_targets = data["player_role_predictions"].reshape(N*B*n_players, n_roles)
            role_backwards_predictions = model_out["role_backwards_prediction"].reshape(N*B*n_players, n_roles)
            loss_backwards_role = self.dm_loss_scale * F.kl_div(
                role_backwards_predictions,
                role_backwards_targets,
                reduction="batchmean",
                log_target=True
            )
            loss += loss_backwards_role
            self.log.watch_mean("loss_role_bp", loss_backwards_role)

        # -------------------------------------------------------------------------
        # Calculate action prediction loss
        # -------------------------------------------------------------------------

        if "action_prediction" in model_out:
            pred = model_out["action_prediction"]
            true = data["player_role_policy"]
            ap_loss = self.dm_loss_scale * calculate_action_prediction_loss(pred, true)
            self.log.watch_mean("ap_loss", ap_loss, display_width=10, display_precision=5)
            loss += ap_loss

        if "action_backwards_prediction" in model_out:
            pred = model_out["action_backwards_prediction"]
            true = data["player_action_predictions"]
            bap_loss = self.dm_loss_scale * calculate_action_prediction_loss(pred, true)
            self.log.watch_mean("bap_loss", bap_loss, display_width=10, display_precision=5)
            loss += bap_loss

        # -------------------------------------------------------------------------
        # Calculate observation prediction loss
        # -------------------------------------------------------------------------

        if self.predicts_observations:
            obs_predictions = model_out["obs_prediction"] # [N, B, n_players, *obs_shape]
            obs_truth = image_to_float(data["player_obs"]) # [N, B, n_players, *obs_shape]

            # remove from prediction other players we are not visible
            if self.dm_vision_filter >= 1:
                visible_obs_predictions = obs_predictions
                visible_obs_truth = obs_truth
            elif self.dm_vision_filter == 0:
                visible_obs_predictions = filter_visible(obs_predictions, player_visible)
                visible_obs_truth = filter_visible(obs_truth, player_visible)
            else:
                pass_through = np.random.choice(a=[True, False], size=player_visible.shape,
                                                p=[self.dm_vision_filter, 1 - self.dm_vision_filter])
                pass_through = torch.tensor(pass_through, dtype=torch.bool,
                                            device=player_visible.device)  # gets cast to uint8 for some reason..
                mask = torch.logical_or(pass_through, player_visible)

                visible_obs_predictions = filter_visible(obs_predictions, mask)
                visible_obs_truth = filter_visible(obs_truth, mask)

            # calculate the loss for the visible players
            pred_obs_mse = F.mse_loss(visible_obs_predictions, visible_obs_truth)

            # use KL if needed
            if self.dm_kl_factor > 0:
                pred_obs_kl = calculate_kl(obs_predictions)/100 # get kl on roughly the same scale as mse
                obs_loss = (1 - self.dm_kl_factor) * pred_obs_mse + self.dm_kl_factor * pred_obs_kl
            else:
                obs_loss = pred_obs_mse

            obs_loss = obs_loss * self.dm_loss_scale
            loss += obs_loss
            self.log.watch_mean("pred_obs_mse", pred_obs_mse)
            self.log.watch_mean("pred_obs_loss", obs_loss)

            # calculate extra stats for logging
            if self.deception_batch_counter % 10 == 9: # doing this late helps with benchmarking
                with torch.no_grad():

                    # log the accuracy on non-visible players (should be worse)
                    pred_nv = filter_visible(obs_predictions, ~player_visible)
                    true_nv = filter_visible(obs_truth, ~player_visible)

                    if len(pred_nv) > 0:
                        # it's possible there are no non visible agents,
                        # there will always be visible agents though as agents can always see themselves
                        obs_mse_nv = F.mse_loss(pred_nv, true_nv)
                        self.log.watch_mean("pred_obs_nv", obs_mse_nv, display_width=0)

                    # note: as agents learn more determanistic policies KL gets larger
                    # would be nice to have some kind of reference level, maybe KL when
                    # observations are rolled along the players dim, ah yes, this will work

                    # also this kl will include non-visible players. This is required as we can't (yet) partially
                    # mask out players for only some transitions within a segment

                    # log the kl between pi(true) and pi(pred) (for true role)
                    kl = calculate_kl(obs_predictions)
                    self.log.watch_mean("pred_obs_kl", kl, display_precision=4, display_width=0)

                    # log the kl between pi(true) and pi(pred) (for true role)
                    rolled_obs = torch.roll(obs_truth, 1, dims=2)
                    kl_reference = calculate_kl(rolled_obs)
                    self.log.watch_mean("ref_kl", kl_reference, display_precision=4)
                    self.log.watch_mean("delta_kl", kl / kl_reference, display_precision=4, display_width=0)

        # -------------------------------------------------------------------------
        # Prediction prediction error
        # -------------------------------------------------------------------------

        if self.predicts_observations:
            n_players = self.vec_env.max_players
            pred_predictions = model_out["obs_backwards_prediction"].reshape(N, B, n_players, *obs_shape)  # [N, B, n_players, *obs_shape]
            true_predictions = image_to_float(data["player_obs_predictions"])
            pred_back_mse = F.mse_loss(pred_predictions, true_predictions)
            self.log.watch_mean("pred_back_mse", pred_back_mse, display_width=0)
            loss += pred_back_mse * self.dm_loss_scale

        # -------------------------------------------------------------------------
        # Apply loss
        # -------------------------------------------------------------------------

        loss = loss * loss_scale
        self.log.watch_mean("dec_loss", loss)

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

        self.deception_batch_counter += 1

    @profiler.record_function("grad_clip")
    def _log_and_clip_grad_norm(self, model, log_var_name="opt_grad"):
        if self.max_grad_norm is not None and self.max_grad_norm != 0:
            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)
        else:
            # even if we don't clip the gradient we should at least log the norm. This is probably a bit slow though.
            # we could do this every 10th step, but it's important that a large grad_norm doesn't get missed.
            grad_norm = 0
            parameters = list(filter(lambda p: p.grad is not None, model.parameters()))
            for p in parameters:
                param_norm = p.grad.data.norm(2)
                grad_norm += param_norm.item() ** 2
            grad_norm = grad_norm ** 0.5
        self.log.watch_mean(log_var_name, grad_norm)

    @profiler.record_function("opt_step")
    def opt_step_minibatch(self, model, optimizer, log_var_name):
        """ Runs the optimization step for a minimatch, forward_minibatch should be called first."""

        if self.amp:
            self.scaler.unscale_(optimizer)
            self._log_and_clip_grad_norm(model, log_var_name)
            self.scaler.step(optimizer)
            self.scaler.update()
            optimizer.zero_grad()
        else:
            self._log_and_clip_grad_norm(model, log_var_name)
            optimizer.step()
            optimizer.zero_grad()

    def _train_model(self, batch_data, model, optimizer, forward_func,
                     epochs:float, mini_batch_size, short_name="model",
                     max_window_size=None, enable_window_offsets=False):
        """
        :param batch_data: a dictionary containing data required for training
        :return:
        """

        for k, v in batch_data.items():
            assert type(v) == np.ndarray, f"batch_data input {k} must be numpy array not {type(v)}"
            assert v.dtype != np.float64, f"batch_data entry {k} is float64, should be float32."

        # convert everything over to torch arrays
        # note: pinning the memory here doesn't seem to help, and would double memory consumption
        for k, v in batch_data.items():
            batch_data[k] = torch.from_numpy(v)

        last_epoch = math.ceil(epochs)

        N, B = batch_data["id"].shape[0:2]
        window_size = min(max_window_size or float('inf'), self.n_steps)

        assert mini_batch_size % window_size == 0
        mini_batch_segments = mini_batch_size // window_size

        mini_batches = B // mini_batch_segments

        assert self.micro_batch_size % window_size == 0
        max_micro_batch_segments = (self.micro_batch_size // window_size)

        # calculate number of micro_batches
        if mini_batch_segments <= max_micro_batch_segments:
            micro_batches = 1
        else:
            assert mini_batch_segments % max_micro_batch_segments == 0
            micro_batches = mini_batch_segments // max_micro_batch_segments

        if self.t == 0:
            print(f" - {short_name} using mini_batches of size {mini_batch_size} ({micro_batches} micro_batches)")

        for i in range(last_epoch):

            segments = list(range(B))
            np.random.shuffle(segments)

            assert mini_batch_segments % micro_batches == 0
            segments_per_micro_batch = mini_batch_segments // micro_batches

            optimizer.zero_grad()

            micro_batch_counter = 0

            # this allows a fraction of an 'epoch' to be processed.
            if i == int(epochs):
                mini_batches_to_process = math.ceil((epochs % 1) * mini_batches)
            else:
                mini_batches_to_process = mini_batches

            for j in range(mini_batches_to_process):
                with autocast(enabled=self.amp):
                    for k in range(micro_batches):
                        batch_start = micro_batch_counter * segments_per_micro_batch
                        batch_end = (micro_batch_counter + 1) * segments_per_micro_batch
                        sample = segments[batch_start:batch_end]

                        # if we are using random window offsets calculate them here...
                        if max_window_size is not None:
                            gap = N - max_window_size
                            if enable_window_offsets and gap > 0:
                                offsets = np.random.randint(0, gap, size=[segments_per_micro_batch])
                            else:
                                enable_window_offsets = False

                        # put together a mini batch from all agents at this time step...
                        mini_batch_data = {}
                        for key, value in batch_data.items():

                            # sampling
                            value = value[:, sample]

                            # select random sub-windows, useful if n_steps if high, but we want small segments
                            if max_window_size is not None:
                                if enable_window_offsets:
                                    # with numpy I could roll the data so each segment had a different start point
                                    # but with torch I need to do it this slower way...
                                    new_value = torch.zeros_like(value[:max_window_size])
                                    for idx in range(len(offsets)):
                                        new_value[:, idx] = value[offsets[idx]:offsets[idx]+max_window_size, idx]
                                    value = new_value
                                else:
                                    value = value[:max_window_size]

                            # rnn states are useful for debuging, but take a lot of v-ram.
                            if key == "rnn_states":
                                # only need first rnn_state for mini_batch, saves a bit of GPU memory
                                value = value[0:1]

                            mini_batch_data[key] = value.to(device=model.device, non_blocking=True)

                        forward_func(mini_batch_data)
                        micro_batch_counter += 1
                self.opt_step_minibatch(model, optimizer, short_name+"_grad")

    @profiler.record_function("train_deception")
    def train_deception(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        this_batch_data = {}
        this_batch_data["roles"] = self.batch_roles
        this_batch_data["t"] = self.batch_t
        this_batch_data["id"] = self.batch_id
        this_batch_data["prev_obs"] = self.batch_prev_obs
        this_batch_data["player_policy"] = self.batch_player_policy
        this_batch_data["player_terminals"] = self.batch_player_terminals
        this_batch_data["player_visible"] = self.batch_player_visible
        this_batch_data["terminals"] = self.batch_terminals
        this_batch_data["player_roles"] = self.batch_player_roles
        this_batch_data["log_policy"] = self.batch_log_policy
        this_batch_data["role_log_policies"] = self.batch_role_log_policies
        this_batch_data["rnn_states"] = self.extract_deception_rnn_states(self.batch_rnn_states)
        if self.batch_player_role_predictions is not None:
            this_batch_data["player_role_predictions"] = self.batch_player_role_predictions
        if self.batch_player_obs is not None:
            this_batch_data["player_obs"] = self.batch_player_obs
        if self.batch_player_obs_predictions is not None:
            this_batch_data["player_obs_predictions"] = self.batch_player_obs_predictions
        if self.batch_player_action_predictions is not None:
            this_batch_data["player_action_predictions"] = self.batch_player_action_predictions
        if self.batch_player_role_policy is not None:
            this_batch_data["player_role_policy"] = self.batch_player_role_policy

        # handle the replay buffer
        self.replay_buffer.append(this_batch_data)
        if len(self.replay_buffer) > self.dm_replay_buffer_multiplier:
            self.replay_buffer = self.replay_buffer[-self.dm_replay_buffer_multiplier:]

        # collate batch data together
        replay_batch_data = {}
        for k in self.replay_buffer[0].keys():
            replay_batch_data[k] = np.concatenate(
                [buffer[k] for buffer in self.replay_buffer],
                axis=1
            )

        # when we use shorter windows it makes sense to run additional epochs as we are only using a portion of the data
        # each time
        windows_per_segment = self.n_steps // (self.dm_max_window_size or self.n_steps)

        self._train_model(
            replay_batch_data,
            self.deception_model,
            self.deception_optimizer,
            self.forward_deception_mini_batch,
            epochs=(self.batch_epochs / len(self.replay_buffer)) * windows_per_segment,
            mini_batch_size=self.dm_mini_batch_size,
            short_name="dec",
            max_window_size=self.dm_max_window_size,
            enable_window_offsets=True
        )

    @profiler.record_function("train_gv")
    def train_global_value(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        batch_data = {}
        batch_data["id"] = self.batch_id
        batch_data["t"] = self.batch_t
        batch_data["roles"] = self.batch_roles
        batch_data["ext_returns"] = self.batch_ext_returns
        batch_data["int_returns"] = self.batch_int_returns
        batch_data["ext_value"] = self.batch_ext_value
        batch_data["int_value"] = self.batch_int_value
        batch_data["player_roles"] = self.batch_player_roles
        batch_data["player_obs"] = self.batch_player_obs
        batch_data["advantages"] = self.batch_advantage
        batch_data["terminals"] = self.batch_terminals
        batch_data["rnn_states"] = self.extract_gv_rnn_states(self.batch_rnn_states)

        # use the same settings as policy
        assert self.batch_size % self.mini_batches == 0
        mini_batch_size = self.batch_size // self.mini_batches
        windows_per_segment = self.n_steps // (self.max_window_size or self.n_steps)

        self._train_model(
            batch_data,
            self.gv_model,
            self.gv_optimizer,
            self.forward_gv_mini_batch,
            epochs=self.batch_epochs * windows_per_segment,
            mini_batch_size=mini_batch_size,
            short_name="gv",
            max_window_size=self.max_window_size,
            enable_window_offsets=True
        )


    @profiler.record_function("train_policy")
    def train_policy(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        batch_data = {}
        batch_data["prev_obs"] = self.batch_prev_obs
        batch_data["id"] = self.batch_id
        batch_data["t"] = self.batch_t
        batch_data["actions"] = self.batch_actions
        batch_data["roles"] = self.batch_roles
        batch_data["ext_returns"] = self.batch_ext_returns
        batch_data["int_returns"] = self.batch_int_returns
        batch_data["ext_value"] = self.batch_ext_value
        batch_data["int_value"] = self.batch_int_value
        batch_data["log_policy"] = self.batch_log_policy
        batch_data["role_log_policy"] = self.batch_role_log_policies # only needed for debugging
        batch_data["player_roles"] = self.batch_player_roles
        batch_data["advantages"] = self.batch_advantage
        batch_data["terminals"] = self.batch_terminals
        batch_data["rnn_states"] = self.extract_policy_rnn_states(self.batch_rnn_states)

        assert self.batch_size % self.mini_batches == 0
        mini_batch_size = self.batch_size // self.mini_batches
        windows_per_segment = self.n_steps // (self.max_window_size or self.n_steps)

        self._train_model(
            batch_data,
            self.policy_model,
            self.policy_optimizer,
            self.forward_policy_mini_batch,
            epochs=self.batch_epochs * windows_per_segment,
            mini_batch_size=mini_batch_size,
            short_name="pol",
            max_window_size=self.max_window_size,
            enable_window_offsets=True
        )

# ------------------------------------------------------------------
# Helper functions
# ------------------------------------------------------------------

def calculate_returns(rewards, dones, final_value_estimate, gamma):
    """
    Calculates returns given a batch of rewards, dones, and a final value estimate.
    Input is vectorized so it can calculate returns for multiple agents at once.
    :param rewards: nd array of dims [N,A]
    :param dones:   nd array of dims [N,A] where 1 = done and 0 = not done for given timestep
    :param final_value_estimate: nd array [A] containing value estimate of final state after last action.
    :param gamma:   discount rate.
    :return:
    """

    N, A = rewards.shape

    returns = np.zeros([N, A], dtype=np.float32)
    current_return = final_value_estimate

    for i in reversed(range(N)):
        returns[i] = current_return = rewards[i] + current_return * gamma * (1.0 - dones[i])

    return returns

def calculate_gae(batch_rewards, batch_value, final_value_estimate, batch_terminal, final_terminal, gamma, lamb):

    N, A = batch_rewards.shape

    batch_advantage = np.zeros_like(batch_rewards, dtype=np.float32)
    prev_adv = np.zeros([A], dtype=np.float32)
    for t in reversed(range(N)):

        if t != N-1:
            is_next_terminal = batch_terminal[t + 1]
            value_next_t = batch_value[t + 1]
        else:
            is_next_terminal = final_terminal
            value_next_t = final_value_estimate

        delta = batch_rewards[t] + gamma * value_next_t * (1.0 - is_next_terminal) - batch_value[t]
        batch_advantage[t] = prev_adv = delta + gamma * lamb * (
                1.0 - is_next_terminal) * prev_adv

    return batch_advantage

def merge_down(x):
    """ Combines first two dims. """
    return x.reshape(-1, *x.shape[2:])

def test_calculate_returns():
    rewards = np.asarray([[0], [2], [1], [4], [1]])
    dones = np.asarray([[0], [0], [0], [1], [0]])
    assert list(calculate_returns(rewards, dones, final_value_estimate=6, gamma=1.0).ravel()) == [7, 7, 5, 4, 7]


def filter_visible(x, visible):
    """
    Note: once filtered reshaping back to [N, B] is no longer viable (as different N's have different B's)
    :param x: Input is in form [N, B, n_players, *]
    :param visible: [N, B, n_players] indicating if player is visible
    :return: flatened filtered results (N*B*n_players <filtered>, *]
    """

    N, B, n_players, *x_shape = x.shape
    assert visible.shape == (N, B, n_players), f"Expected visible to be {N, B, n_players} but found {visible.shape}"

    visible = visible.reshape(-1)
    return x.reshape(N * B * n_players, *x_shape)[visible].reshape(-1, *x_shape)

def test_filter_visible():
    data = torch.zeros([2,4,3,1])
    for n in range(2):
        for b in range(4):
            for p in range(3):
                data[n,b,p] = n*100+b*10+p
    visible = torch.tensor([
        [[True, True, True],
        [True, True, True],
        [True, True, True],
        [True, True, True]],

        [[True, True, True],
         [True, True, True],
         [True, True, True],
         [True, True, True]]
    ])

    test_1 = filter_visible(data, visible)
    assert test_1.shape == (2*4*3, 1)

    visible[0,1,2] = False
    test_2 = filter_visible(data, visible)
    assert test_2.shape == (2*4*3-1, 1)
    assert 12 not in test_2

def image_to_float(x):
    """ Convert image from uint8 to float, scaled to 0..1. """
    assert x.dtype == torch.uint8
    return x.float() * (1/255.0)

def image_to_uint8(x):
    """ Convert image from float in range [0..1] to uint8. """
    assert x.dtype == torch.float32
    return torch.clamp(x * 255.0, 0, 255).to(torch.uint8)

def calculate_action_prediction_loss(pred: torch.Tensor, true: torch.Tensor):
    """

    :param pred: tensor of logprobs of dims [N, B, n_players, n_roles, n_actions]
    :param true: tensor of logprobs of dims [N, B, n_players, n_roles, n_actions]
    :return: scalar indicating average kl over batch.
    """
    assert pred.shape == true.shape
    N, B, n_players, n_roles, n_actions = pred.shape
    pred = pred.reshape(N * B * n_players * n_roles, n_actions)
    true = true.reshape(N * B * n_players * n_roles, n_actions)
    return F.kl_div(pred, true, reduction='batchmean', log_target=True)


def calculate_roll_prediction_nll(role_predictions, role_targets, filter=None):
    """
    Calculate KL divergence between role predictions and role targets.
    :param role_predictions: Prediction of role for each player
        i.e, at timestep N, player B's prediction about n_players role.
        log probabilities for each player, float32 tensor of dims [N, B, n_players, n_roles]
    :param role_targets: true roles, int64 tensor of dims [N, B, n_players]
    :param filter: (optional) boolean tensor indicating which items in batch to use of dims [N, B]
    :return:
    """


    role_targets = merge_down(role_targets)
    role_predictions = merge_down(role_predictions)

    if filter is not None:
        filter = merge_down(filter)
        if sum(filter) == 0:
            return None
        role_predictions = role_predictions[filter]
        role_targets = role_targets[filter]
    # we want this in N*B, n_roles, n_players order as torch expects, (batch, class, d1,d2,d3... etc)
    role_predictions = role_predictions.transpose(1, 2)
    return F.nll_loss(role_predictions, role_targets, reduction='none')


def calculate_deception_bonus_from_actions(
        player_action_prediction_predictions: torch.Tensor,
        prior_role_belief: torch.Tensor,
        true_role: torch.Tensor,
        mask: torch.Tensor,
        actions: torch.Tensor,
        device: str,
    ):
    """
    Calculates a deception bonus based on modifying other agents belief under the assumption that they are
    bayesian agents, and using our own estimations of their estimations of our likely actions for each role.

    The observations version uses the policy of the predicted observations, but this one uses predictions of policy
    directly. This avoids rnn_states, and having to predict unimportant parts of the observation space.

    :param player_action_prediction_predictions: distribution over actions that represent agent i's belief about agent j's
        belief about i's action distribution for each role at current timestep. These should be log probabilities
        Tensor of dims [B, n_players, n_roles, *action_space] of type float32
    :param prior_role_belief: Agent i's belief about agent j's belief about agent i's true role. These should be log probabilities
        Tensor of dims [B, n_players, n_roles] of type float32

    :param true_role: The true roles for each player, of shape [B] of type long
    :param mask: A mask indicating which players we are trying to deceive, and therefore get a bonus for.
        This can be used to mask out invisible players, or players that are known to be on the same team
        Tensor of dims [B, n_players] of type float

    :param actions: Actions for each agent
        Tensor of dims [B] of type long

    :return: The bonus for each player summed over other players, tensor of dims [B],
    """

    # upload everything to correct device, and get dtype right
    player_action_prediction_predictions = player_action_prediction_predictions.to(device)
    prior_role_belief = prior_role_belief.to(device)
    true_role = true_role.to(device)
    mask = mask.to(device)
    actions = actions.to(device).to(torch.long)

    B, n_players, *role_policy_shape = player_action_prediction_predictions.shape
    n_roles = 3

    # [B, n_players, n_roles, n_actions]
    role_policy = torch.exp(player_action_prediction_predictions)
    prior_role_belief = torch.exp(prior_role_belief)

    # next calculate the B_ji, which is agent j's belief about agent i's role being the true role if we take
    # action a
    # (actually this is not the posterior belief, it's the likelihood ratio)
    likelihood_true_role = torch.zeros([B, n_players], device=device)

    for j in range(n_players):
        role_sum = torch.zeros([B], device=device)
        for role in range(n_roles):
            role_sum += role_policy[range(B), j, role, actions] * prior_role_belief[:, j, role]
        # we add 1e-6 just for stability.
        likelihood_true_role[:, j] = role_policy[range(B), j, true_role, actions] / (role_sum + 1e-6)

    # bonus is just neg log likelihood, and sum over other players we are trying to deceive
    # clip it so we can't get extremely high bonus
    #bonus = -torch.clip(torch.log(likelihood_true_role), -10, +10)
    bonus = -torch.log(likelihood_true_role)
    bonus = bonus * mask
    bonus = torch.sum(bonus, dim=-1) # sum over players
    return bonus

def test_deception_bonus():

    # uniform belief, there should be no deception
    # order is i's belief about j's belief about i in role r taking action a
    # [i,j,r,a], dims should be 2,2,3,2
    prediction_predictions = torch.Tensor(np.asarray(
        [[
            [[0.75, 0.5], [0.5, 0.5], [0.5, 0.5]],  # P0 thinks P0 thinks P0 taking action 0 is a 'red' move
            [[0.75, 0.5], [0.5, 0.5], [0.5, 0.5]], # P0 thinks P1 thinks P0 taking action 0 is a 'red' move
        ],
        [
            [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]],  # R
            [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]],  # G

        ]],
    ))

    # this is of shape [2,2,3] [i, j, r]
    prior_role_belief = torch.Tensor(np.asarray([
        [
            # 0 belief about 0 belief about roles
            [1, 0, 0],
            # 0 belief about 1 belief about roles
            [1/3, 1/3, 1/3],
        ],
        [
            # 1 belief about 0 belief about roles
            [1/3, 1/3, 1/3],
            # 1 belief about 1 belief about roles
            [0, 1, 0],
        ],
    ]))

    # true roles
    true_role = torch.Tensor(np.asarray([0, 1])).to(torch.long)

    # we have only 2 actions in this game, this should be 2
    actions = torch.Tensor(np.asarray([0, 1]))

    # no need to mask out ourselves as prior does this for us.
    mask = torch.Tensor(np.asarray([[1, 1], [1, 1]]))

    result = calculate_deception_bonus_from_actions(
        player_action_prediction_predictions = torch.log(prediction_predictions),
        prior_role_belief = torch.log(prior_role_belief),
        true_role = true_role,
        mask = mask,
        actions = actions,
        device = 'cpu'
    )

    assert abs(result[0] - -0.25) < 0.1 and abs(result[1] - 0) < 1e-4, f"{result}, (0.25, 0)"

test_deception_bonus()
test_calculate_returns()
test_filter_visible()