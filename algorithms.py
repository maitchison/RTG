"""
PPO implementation for Multi-Agent Reinforcement Learning

Planned Features

[ ] Role Prediction
[ ] Prediction of other players observations
[ ] Prediction of other players prediction of our own observation
[ ] Optional Shared value function during training
[ ] micro batching for low GPU machines (using gradient accumulation)

Notation

b: number of agents calculated simultaneously (effectively the batch size used during rollout)
p: (max) number of players in game
r: number of roles
a: number of actions

batch: the total batch of data generated by a rollout, i.e. 64k
minibatch: the batch size used during optimizer updates, often batch/4, e.g. 16k
microbatch: smaller batches used due to GPU memory limitations, gradients are accumulated during microbatch processing
    making them equivalent to a larger minibatch size, but with less memory (this does not allow for batch-normalizion
    however)

extrinsic and intrinsic rewards are learned separately as value_int, and value_ext

"""

import torch.nn.functional as F
import pickle
import gzip
import time
import os

from torch.cuda.amp import GradScaler, autocast
from logger import Logger
from typing import Union

from models import *

from marl_env import MultiAgentVecEnv
import torch.autograd.profiler as profiler

import platform
# this enable text colors on windows
if platform.system() == "Windows":
    import colorama
    colorama.init()

class MarlAlgorithm():

    def __init__(self, N, A, model: nn.Module):

        self.policy_model = model

        self.batch_size = int()
        self.n_steps = N
        self.n_agents = A

        self.obs_shape = self.policy_model.input_dims
        self.rnn_state_shape = [2, self.policy_model.memory_units]  # records h and c for LSTM units
        self.policy_shape = [self.policy_model.actions]

    def learn(self, total_timesteps, reset_num_timesteps=True):
        raise NotImplemented()

    @staticmethod
    def load(filename, env):
        raise NotImplemented()

    def get_initial_state(self, n_agents=None):
        n_agents = n_agents or self.n_agents
        return torch.zeros([n_agents, *self.rnn_state_shape], dtype=torch.float32, device=self.policy_model.device)

    def forward(self, obs, rnn_state):
        raise NotImplemented()

class PMAlgorithm(MarlAlgorithm):
    """
    Handles rollout generation and training on rollout data.
    """

    def __init__(
            self,
            vec_env:MultiAgentVecEnv,  # the environment this agent acts in
            name="agent",              # name of agent
            verbose=False,             # set to true to enable debug printing

            enable_deception=False,    # enables the deception model
            export_rollout=False,      # writes rollouts to disk (very large!)

            # ------ model parameters ------------

            amp=False,  # automatic mixed precision
            device="cpu",
            memory_units=128,
            out_features=128,
            model_name="default",
            micro_batch_size: Union[str, int] = "auto",

            # ------ long list of parameters to algorithm ------------
            n_steps=32,
            learning_rate=2.5e-4,
            adam_epsilon=1e-5,
            normalize_advantages=True,
            gamma=0.99,
            gamma_int=0.99,
            gae_lambda=0.95,
            batch_epochs=4,
            entropy_bonus=0.01,
            mini_batches=4,
            use_clipped_value_loss=False,  # shown to be not effective
            vf_coef=0.5,
            ppo_epsilon=0.2,
            max_grad_norm=5.0,
        ):

        A = vec_env.total_agents
        N = n_steps

        self.enable_deception = enable_deception
        self.export_rollout = export_rollout

        if device == "data_parallel":
            device = "cuda"
            data_parallel = True
        else:
            data_parallel = False

        model = PolicyModel(vec_env, device=device, memory_units=memory_units, model=model_name,
                        data_parallel=data_parallel, out_features=out_features)

        if self.enable_deception:
            self.deception_model = DeceptionModel(vec_env, device=device, memory_units=memory_units, model=model_name,
                        data_parallel=data_parallel, out_features=out_features)
        else:
            self.deception_model = None


        super().__init__(n_steps, A, model)

        if self.enable_deception:
            # normally the rnn_states are h,c, but with the deception model we use h, c, h_deception, c_deception
            # this means both models must have the same number of memory units but that's fine.
            self.rnn_state_shape[0] *= 2

        self.name = name
        self.amp = amp
        self.data_parallel = data_parallel
        self.verbose = verbose
        self.log = Logger()
        self.vec_env = vec_env

        self.normalize_advantages = normalize_advantages
        self.gamma = gamma
        self.gamma_int = gamma_int
        self.gae_lambda = gae_lambda
        self.batch_epochs = batch_epochs
        self.entropy_bonus = entropy_bonus
        self.mini_batches = mini_batches
        self.use_clipped_value_loss = use_clipped_value_loss
        self.vf_coef = vf_coef
        self.ppo_epsilon = ppo_epsilon
        self.max_grad_norm = max_grad_norm

        # these are not used yet
        self.intrinsic_reward_propagation = False,
        self.normalize_intrinsic_rewards = False
        self.extrinsic_reward_scale = 1.0
        self.intrinsic_reward_scale = 1.0
        self.log_folder = "."

        self.policy_optimizer = torch.optim.Adam(self.policy_model.parameters(), lr=learning_rate, eps=adam_epsilon)
        if self.deception_model is not None:
            self.deception_optimizer = torch.optim.Adam(self.deception_model.parameters(), lr=learning_rate,
                                                        eps=adam_epsilon)

        self.t = 0
        self.learn_steps = 0
        self.batch_size = N * A

        self.episode_score = np.zeros([A], dtype=np.float32)
        self.episode_len = np.zeros([A], dtype=np.int32)

        # ------------------------------------
        # current agent state

        # agents most previously seen observation
        self.agent_obs = np.zeros([A, *self.obs_shape], dtype=np.uint8)
        # agents current rnn state
        self.agent_rnn_state = torch.zeros([A, *self.rnn_state_shape], dtype=torch.float32, device=self.policy_model.device)

        # ------------------------------------
        # the rollout transitions (batch)

        # rnn_state of the agent before rollout begins
        self.prev_rnn_states = np.zeros([A, *self.rnn_state_shape], dtype=np.float32)
        # observation of the agent before taking action
        self.batch_prev_obs = np.zeros([N, A, *self.obs_shape], dtype=np.uint8)
        # observation of the agent after taking action
        self.next_obs = np.zeros([N, A, *self.obs_shape], dtype=np.uint8) # note, do I need these?
        # action taken by agent to generate the transition
        self.batch_actions = np.zeros([N, A], dtype=np.int64)
        # log_policy that generated the action
        self.batch_log_policy = np.zeros([N, A, *self.policy_shape], dtype=np.float32)
        # float, batch_terminals[t] indicates if timestep t+1 was terminal or not
        self.batch_terminals = np.zeros([N, A], dtype=np.float32)
        # the current terminal state of each agent
        self.terminals = np.zeros([A], dtype=np.float32)
        # rewards generated at transition
        self.batch_ext_rewards = np.zeros([N, A], dtype=np.float32)
        self.batch_int_rewards = np.zeros([N, A], dtype=np.float32)
        # value estimates by agent just before action was taken
        self.batch_ext_value = np.zeros([N, A], dtype=np.float32)
        self.batch_int_value = np.zeros([N, A], dtype=np.float32)
        # return estimates (from which state though?)
        self.batch_ext_returns = np.zeros([N, A], dtype=np.float32)
        self.batch_int_returns = np.zeros([N, A], dtype=np.float32)

        # the true role of each player in the game that agent a is playing
        self.batch_roles = np.zeros([N, A, vec_env.max_players], dtype=np.int64)
        # ground truth observations for each player in public_id order,
        # this is so when we take a random sample we always have the information we need.
        # It's a bit wasteful with memory though, especially with high player counts.
        if self.enable_deception:
            self.batch_player_obs = np.zeros([N, A, vec_env.max_players, *self.obs_shape], dtype=np.uint8)

        # advantage used during policy gradient (combination of intrinsic and extrinsic reward)
        self.batch_advantage = np.zeros([N, A], dtype=np.float32)

        self.ext_final_value_estimate = np.zeros([A], dtype=np.float32)
        self.int_final_value_estimate = np.zeros([A], dtype=np.float32)

        self.intrinsic_returns_rms = utils.RunningMeanStd(shape=())

        # get micro batch size
        self.micro_batches = 0

        if utils.try_cast_to_int(micro_batch_size) is not None:
            micro_batch_size = utils.try_cast_to_int(micro_batch_size)
        elif micro_batch_size.lower() == "auto":
            micro_batch_size = self._get_auto_micro_batch_size()
        elif micro_batch_size.lower() == "off":
            micro_batch_size = self.mini_batch_size
        else:
            raise ValueError(f"Invalid micro batch size {micro_batch_size}")

        self.micro_batches = max(1, self.mini_batch_size // micro_batch_size)

        if self.micro_batches != 1:
            print(f" -using {self.micro_batches} micro_batches of size {micro_batch_size}")

        self.scaler = GradScaler() if self.amp else None

    @property
    def mini_batch_size(self):
        return self.batch_size // self.mini_batches

    def save(self, filename):

        data = {
            'step': self.t,
            'model_state_dict': self.policy_model.state_dict(),
            'optimizer_state_dict': self.policy_optimizer.state_dict()
        }

        if self.enable_deception:
            data['deception_model_state_dict'] = self.deception_model.state_dict()
            data['deception_optimizer_state_dict'] = self.deception_optimizer.state_dict()

        if self.normalize_intrinsic_rewards:
            data['ems_norm'] = self.ems_norm
            data['intrinsic_returns_rms'] = self.intrinsic_returns_rms,

        torch.save(data, filename)

        base_path = os.path.split(filename)[0]

        with open(os.path.join(base_path, "checkpoint_log.dat"), "wb") as f:
            pickle.dump(self.log, f)

        self.log.export_to_csv(os.path.join(base_path, "training_log.csv"))


    def _eval(self):
        self.policy_model.eval()
        if self.deception_model is not None:
            self.deception_model.eval()

    def _train(self):
        self.policy_model.train()
        if self.deception_model is not None:
            self.deception_model.train()

    def _export_rollout(self, filename):
        """
        Write rollout to disk
        """
        # compression level 1 gives a lot of compression for our data, and is very fast.
        # level 9 is only 10% better, but 20x slower.
        with gzip.open(filename, "wb", compresslevel=1) as f:
            pickle.dump(self.batch_prev_obs, f)

    @profiler.record_function("learn")
    def learn(self, total_timesteps, reset_num_timesteps=True):

        if reset_num_timesteps:
            self.t = 0
            self.reset()
            self.learn_steps = 0

        batches = total_timesteps // self.batch_size
        for batch in range(batches):

            self.log.watch("epoch", self.t/1e6, display_width=8, display_priority=3, display_precision=1)

            start_time = time.time()
            self._eval()
            self.generate_rollout()
            self._train()
            self.train_policy()
            if self.enable_deception:
                self.train_deception()

            end_time = time.time()
            step_time = end_time - start_time
            # measure number of agent interactions with environment per second
            self.log.watch_mean("ips", self.batch_size / step_time, history_length=4, display_width=8,
                                display_priority=2, display_precision=0)

            if self.export_rollout:
                self._export_rollout(os.path.join(self.log_folder, f"rollout_{self.learn_steps}.dat"))

            if self.verbose:
                if self.learn_steps % 10 == 0:
                    print(f"{self.learn_steps}, {self.t / 1000 / 1000:.1f}M")
                self.log.print_variables(include_header=self.learn_steps % 10 == 0)

            self.t += self.batch_size
            self.learn_steps += 1

            self.log.record_step()

    def _get_auto_micro_batch_size(self):
        """ Returns an appropriate micro-batch size. """

        # working this out will be complex
        # maybe can assume that 12GB gives us 4096 with AMP and 2048 without
        if type(self.policy_model._encoder) is DefaultEncoder:
            micro_batch_size = 4096
        elif type(self.policy_model._encoder) is FastEncoder:
            micro_batch_size = 8192
        else:
            micro_batch_size = 1024 # just a guess

        if self.enable_deception:
            micro_batch_size //= 2

        if self.amp:
            # amp seems to work very poorly for large batchsizes (perhaps due to the issue with adding lots of small
            # gradients together. If we keep the micro_batch_size small it works ok, but performance is not great.
            micro_batch_size = min(1024, micro_batch_size)
        if self.data_parallel:
            micro_batch_size *= 4 # should be number of GPUs

        print(f" -using max micro-batch size of {micro_batch_size}")

        return micro_batch_size

    def reset(self):
        # initialize agent
        self.agent_obs = self.vec_env.reset()
        self.agent_rnn_state *= 0
        self.episode_score *= 0
        self.episode_len *= 0
        self.terminals *= 0

    @profiler.record_function("generate_rollout")
    def generate_rollout(self):
        """
        Runs agents through environment for n_steps steps, recording the transitions as it goes.
        :return: Nothing
        """
        with torch.no_grad():

            # make a copy of rnn_states at beginning of rollout, which we will use to initialize the rnn state.
            self.prev_rnn_states[:] = self.agent_rnn_state.cpu()[:]

            for t in range(self.n_steps):

                # performance:
                # around 10ms to forward the model on GPU, and 30ms to step the environment
                # this is for 256 players, which means env only runs at ~1000FPS (it improves with more players though)

                prev_obs = self.agent_obs.copy() #[A, *obs_shape]
                prev_terminals = self.terminals.copy()

                if self.enable_deception:

                    # ------------------------------------
                    # roles
                    batch_roles = self.vec_env.get_roles_expanded()
                    self.batch_roles[t] = batch_roles

                    # ------------------------------------
                    # observations

                    # get order for observations
                    orderings = []
                    for game in self.vec_env.games:
                        game_players = [(player.public_id, player.index) for player in game.players]
                        game_players.sort()
                        order = [index for public_id, index in game_players]
                        orderings.append(order)

                    # agent_obs is [n_games*n_players, *obs_shape]
                    # get batch_player_obs and reshape to [n_games, n_players, *obs_shape]
                    batch_player_obs = self.agent_obs.copy().reshape(len(self.vec_env.games), self.vec_env.max_players, *self.obs_shape)

                    # put observations into public_id order
                    # todo: find a tensor operation to do this
                    for i in range(len(batch_player_obs)):
                        batch_player_obs[i, :] = batch_player_obs[i, orderings[i]]

                    # make batch_player_obs [A, n_players, *obs_shape] by duplicating for each player
                    batch_player_obs = batch_player_obs.repeat(self.vec_env.max_players, axis=0)

                    # stub: zero out other players observations
                    n_players = self.vec_env.max_players
                    for i in range(len(batch_player_obs)):
                        for j in range(n_players):
                            if (i % self.vec_env.max_players) != orderings[i // self.vec_env.max_players][j]:
                                if j == 0:
                                    c = (200,0,0,0,0)
                                elif j == 1:
                                    c = (0,200,0,0,0)
                                elif j == 2:
                                    c = (0,0,200,0,0)
                                else:
                                    c = (128,128,0,0,0)
                                c = np.asarray(c).reshape([1,1,5])
                                batch_player_obs[i, j] = c

                    self.batch_player_obs[t] = batch_player_obs

                # forward state through model, then detach the result and convert to numpy.
                with profiler.record_function("gr_model_step"):
                    model_out, new_agent_rnn_state = self.forward(self.agent_obs, self.agent_rnn_state)
                self.agent_rnn_state[:] = new_agent_rnn_state

                log_policy = model_out["log_policy"].detach().cpu().numpy()
                ext_value = model_out["ext_value"].detach().cpu().numpy()

                # sample actions and run through environment.
                actions = np.asarray([utils.sample_action_from_logp(prob) for prob in log_policy], dtype=np.int32)
                with profiler.record_function("gr_env_step"):
                    self.agent_obs, ext_rewards, new_terminals, infos = self.vec_env.step(actions)

                self.terminals[:] = new_terminals

                # work out our intrinsic rewards
                int_value = model_out["int_value"].detach().cpu().numpy()
                int_rewards = np.zeros_like(ext_rewards)

                # save raw rewards for monitoring the agents progress
                raw_rewards = np.asarray([info.get("raw_reward", reward) for reward, info in zip(ext_rewards, infos)],
                                         dtype=np.float32)

                self.episode_score += raw_rewards
                self.episode_len += 1

                for i, terminal in enumerate(self.terminals):
                    if terminal:
                        # reset the internal state on episode completion
                        self.agent_rnn_state[i] *= 0
                        # reset is handled automatically by vectorized environments
                        # so just need to keep track of book-keeping
                        self.log.watch_full("ep_score", self.episode_score[i])
                        self.log.watch_full("ep_length", self.episode_len[i])
                        self.episode_score[i] = 0
                        self.episode_len[i] = 0

                with profiler.record_function("gr_copy"):
                    self.batch_prev_obs[t] = prev_obs
                    self.next_obs[t] = self.agent_obs
                    self.batch_actions[t] = actions
                    self.batch_ext_rewards[t] = ext_rewards
                    self.batch_log_policy[t] = log_policy
                    self.batch_terminals[t] = prev_terminals
                    self.batch_int_rewards[t] = int_rewards
                    self.batch_ext_value[t] = ext_value
                    self.batch_int_value[t] = int_value

            # get value estimates for final observation.
            model_out, _ = self.forward(self.agent_obs, self.agent_rnn_state)

            self.ext_final_value_estimate = model_out["ext_value"].detach().cpu().numpy()
            if "int_value" in model_out:
                self.int_final_value_estimate = model_out["int_value"].detach().cpu().numpy()

        self.calculate_returns()

    def calculate_returns(self):
        """
        Calculate returns from previous rollout.
        Updates the following variables
            ext_returns, int_returns
            ext_advantage
        :return:
        """

        ext_advantage = calculate_gae(self.batch_ext_rewards, self.batch_ext_value, self.ext_final_value_estimate,
            self.batch_terminals, self.terminals, self.gamma, self.gae_lambda)
        self.batch_ext_returns = ext_advantage + self.batch_ext_value

        # calculate the intrinsic returns, but let returns propagate through terminal states.
        if self.normalize_intrinsic_rewards:
            # normalize returns using EMS
            for t in range(self.n_steps):
                self.ems_norm = 0.99 * self.ems_norm + self.batch_int_rewards[t, :]
                self.intrinsic_returns_rms.update(self.ems_norm.reshape(-1))
            # normalize the intrinsic rewards
            # we multiply by 0.4 otherwise the intrinsic returns sit around 1.0, and we want them to be more like 0.4,
            # which is approximately where normalized returns will sit.
            self.intrinsic_reward_norm_scale = (1e-5 + self.intrinsic_returns_rms.var ** 0.5)
            scaled_int_rewards = self.batch_int_rewards / self.intrinsic_reward_norm_scale * 0.4
        else:
            self.intrinsic_reward_norm_scale = 1
            scaled_int_rewards = self.batch_int_rewards

        int_advantage = calculate_gae(scaled_int_rewards, self.batch_int_value, self.int_final_value_estimate,
                                           self.batch_terminals * self.intrinsic_reward_propagation,  # let rewards through
                                           self.terminals * self.intrinsic_reward_propagation,
                                           self.gamma_int, self.gae_lambda)
        self.batch_int_returns = int_advantage + self.batch_int_value

        # sum them together
        self.batch_advantage = self.extrinsic_reward_scale * ext_advantage + self.intrinsic_reward_scale * int_advantage
        if self.normalize_advantages:
            self.batch_advantage = (self.batch_advantage - self.batch_advantage.mean()) / (self.batch_advantage.std() + 1e-8)


        self.log.watch_mean("adv_mean", np.mean(self.batch_advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_std", np.std(self.batch_advantage), display_width=0 if self.normalize_advantages else 10)
        self.log.watch_mean("adv_max", np.max(self.batch_advantage), display_width=0 if self.normalize_advantages else 0)
        self.log.watch_mean("adv_min", np.min(self.batch_advantage), display_width=0 if self.normalize_advantages else 0)
        self.log.watch_mean("batch_reward_ext", np.mean(self.batch_ext_rewards), display_name="rew_ext", display_width=0)
        self.log.watch_mean("batch_return_ext", np.mean(self.batch_ext_returns), display_name="ret_ext")
        self.log.watch_mean("batch_return_ext_std", np.std(self.batch_ext_returns), display_name="ret_ext_std",
                            display_width=0)
        self.log.watch_mean("value_est_ext", np.mean(self.batch_ext_value), display_name="est_v_ext")
        self.log.watch_mean("value_est_ext_std", np.std(self.batch_ext_value), display_name="est_v_ext_std", display_width=0)
        self.log.watch_mean("ev_ext", utils.explained_variance(self.batch_ext_value.ravel(), self.batch_ext_returns.ravel()))

        # these are not helpful yet...
        # if self.enable_deception:
        #     self.log.watch_mean("batch_reward_int", np.mean(self.int_rewards), display_name="rew_int", display_width=0)
        #     self.log.watch_mean("batch_reward_int_std", np.std(self.int_rewards), display_name="rew_int_std",
        #                         display_width=0)
        #     self.log.watch_mean("batch_return_int", np.mean(self.batch_int_returns), display_name="ret_int")
        #     self.log.watch_mean("batch_return_int_std", np.std(self.batch_int_returns), display_name="ret_int_std")
        #     self.log.watch_mean("batch_return_int_raw_mean", np.mean(self.int_returns_raw),
        #                         display_name="ret_int_raw_mu",
        #                         display_width=0)
        #     self.log.watch_mean("batch_return_int_raw_std", np.std(self.int_returns_raw),
        #                         display_name="ret_int_raw_std",
        #                         display_width=0)
        #
        #     self.log.watch_mean("value_est_int", np.mean(self.batch_int_value), display_name="est_v_int")
        #     self.log.watch_mean("value_est_int_std", np.std(self.batch_int_value), display_name="est_v_int_std")
        #     self.log.watch_mean("ev_int", utils.explained_variance(self.batch_int_value.ravel(), self.batch_int_returns.ravel()))
        #
        #     if self.normalize_intrinsic_rewards:
        #         self.log.watch_mean("norm_scale_int", self.intrinsic_reward_norm_scale, display_width=12)

    @profiler.record_function("model_forward")
    def forward(self, obs, rnn_states):
        """
        Forwards a single observations through model for each agent
        For more advanced uses call model.forward_sequence directly.

        :param obs: Observations for each agent, tensor of dims [B, *observation_shape]
        :param rnn_states: tensor of dims [B, 2|4, memory_units]

        :return: a tuple(
            dictionary containing,
                'log_policy',       [B, actions]
                'ext_value', and    [B]
                'int_value'.        [B],
            new_rnn_states          [B, 2|4, memory_dims]

        """

        # make a sequence of length 1 and forward it through model
        result, new_rnn_states = self.policy_model.forward_sequence(
            obs=obs[np.newaxis],
            rnn_states=rnn_states[:, 0:2]
        )

        if self.enable_deception:
            deception_result, new_deception_rnn_states = self.deception_model.forward_sequence(
                obs=obs[np.newaxis],
                rnn_states=rnn_states[:, 2:4]
            )
            for k, v in deception_result.items():
                result[k] = v
            new_rnn_states = torch.cat([new_rnn_states, new_deception_rnn_states], dim=1)

        # remove the sequence of length 1
        unsqueze_result = {}
        for k, v in result.items():
            unsqueze_result[k] = v[0]

        return unsqueze_result, new_rnn_states

    @profiler.record_function("minibatch_forward")
    def forward_policy_mini_batch(self, data, rnn_states):
        """
        Run mini batch through model generating loss, call opt_step_mini_batch to apply the update.
        mini batch should have the structure [N, B, *] where N is the sequence length, and is the batch length

        :param data: dictionary containing data for
        :param rnn_states: current rnn_states of dims [B, 2, memory_units]
        """

        loss = torch.tensor(0, dtype=torch.float32, device=self.policy_model.device)

        # -------------------------------------------------------------------------
        # Calculate loss_pg
        # -------------------------------------------------------------------------

        prev_obs = data["prev_obs"]
        terminals = data["terminals"]

        # these were provided in N, B format but we want them just as one large batch.
        actions = merge_down(data["actions"])
        policy_log_probs = merge_down(data["log_policy"])
        advantages = merge_down(data["advantages"])
        # stub: put this back in
        weights = 1

        N, B, *obs_shape = prev_obs.shape

        with profiler.record_function("fmb_forward_model"):
            model_out, _ = self.policy_model.forward_sequence(
                obs=prev_obs,
                rnn_states=rnn_states,
                terminals=terminals
            )

        # output will be a sequence of [N, B] but we want this just as [N*B] for processing
        for k, v in model_out.items():
            model_out[k] = merge_down(v)

        logps = model_out["log_policy"]

        actions = actions.to(dtype=torch.int64) # required for indexing

        ratio = torch.exp(logps[range(N*B), actions] - policy_log_probs[range(N*B), actions])
        clipped_ratio = torch.clamp(ratio, 1 - self.ppo_epsilon, 1 + self.ppo_epsilon)

        loss_clip = torch.min(ratio * advantages, clipped_ratio * advantages)
        loss_clip_mean = (weights*loss_clip).mean()

        self.log.watch_mean("loss_pg", loss_clip_mean, history_length=64)
        loss += loss_clip_mean

        # -------------------------------------------------------------------------
        # Calculate loss_value
        # -------------------------------------------------------------------------

        value_heads = ["ext", "int"]

        for value_head in value_heads:
            value_prediction = model_out["{}_value".format(value_head)]
            returns = merge_down(data["{}_returns".format(value_head)])
            old_pred_values = merge_down(data["{}_value".format(value_head)])

            if self.use_clipped_value_loss:
                # is is essentially trust region for value learning, and seems to help a lot.
                value_prediction_clipped = old_pred_values + torch.clamp(value_prediction - old_pred_values,
                                                                         -self.ppo_epsilon, +self.ppo_epsilon)
                vf_losses1 = (value_prediction - returns).pow(2)
                vf_losses2 = (value_prediction_clipped - returns).pow(2)
                loss_value = -self.vf_coef * 0.5 * torch.mean(torch.max(vf_losses1, vf_losses2) * weights)
            else:
                # simpler version, just use MSE.
                vf_losses1 = (value_prediction - returns).pow(2)
                loss_value = -self.vf_coef * 0.5 * torch.mean(vf_losses1 * weights)

            self.log.watch_mean("loss_v_" + value_head, loss_value, history_length=64)
            loss += loss_value

        # -------------------------------------------------------------------------
        # Calculate loss_entropy
        # -------------------------------------------------------------------------

        loss_entropy = -(logps.exp() * logps).sum(axis=1)
        loss_entropy = (loss_entropy * weights * self.entropy_bonus).mean()
        self.log.watch_mean("loss_ent", loss_entropy)
        loss += loss_entropy

        # -------------------------------------------------------------------------
        # Apply loss
        # -------------------------------------------------------------------------

        self.log.watch_mean("loss", loss)
        loss = loss / self.micro_batches

        # -------------------------------------------------------------------------
        # Run optimizer
        # -------------------------------------------------------------------------

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(-loss).backward()
            self.log.watch_mean("s_unskip", self.scaler._get_growth_tracker())
            self.log.watch_mean("s_scale", self.scaler.get_scale())
        else:
            (-loss).backward()

    def forward_deception_mini_batch(self, data, deception_rnn_states):

        loss = torch.tensor(0, dtype=torch.float32, device=self.policy_model.device)

        prev_obs = data["prev_obs"]
        terminals = data["terminals"]
        N, B, *obs_shape = prev_obs.shape

        model_out, _ = self.deception_model.forward_sequence(
            obs=prev_obs,
            rnn_states=deception_rnn_states,
            terminals=terminals
        )

        # -------------------------------------------------------------------------
        # Calculate loss_role
        # -------------------------------------------------------------------------

        role_targets = merge_down(data["roles"]) # [N*B, n_players]
        # predictions come out as [N*B, n_players, n_roles], but we need them as [N*B, n_roles, n_players]
        rp_coef = 1.0 * 0.0 # stub no role prediction
        role_predictions = merge_down(model_out["role_prediction"]).transpose(1, 2)
        loss_role = rp_coef * torch.nn.functional.nll_loss(role_predictions, role_targets)

        loss += loss_role
        self.log.watch_mean("loss_role", loss_role)

        # -------------------------------------------------------------------------
        # Calculate observation prediction loss
        # -------------------------------------------------------------------------

        # note, might be better to do the MSE part on the CPU as this will require *a lot* of ram.
        ob_coef = 4.0
        obs_predictions = model_out["obs_prediction"] # [N*B, n_players, *obs_shape]
        obs_truth = merge_down(data["player_obs"]) # [N*B, n_players, *obs_shape] (in public_id order)
        obs_truth = obs_truth.float()/255
        # stub disable x,y prediction
        loss_obs_prediction = ob_coef * F.mse_loss(
            obs_predictions.reshape(-1, *obs_shape)[:, :, :, :3],
            obs_truth.reshape(-1, *obs_shape)[:, :, :, :3]
        )
        loss += loss_obs_prediction
        self.log.watch_mean("loss_obs", loss_obs_prediction)

        # output will be a sequence of [N, B] but we want this just as [N*B] for processing
        for k, v in model_out.items():
            model_out[k] = merge_down(v)

        # -------------------------------------------------------------------------
        # Apply loss
        # -------------------------------------------------------------------------

        self.log.watch_mean("dec_loss", loss)
        loss = loss / self.micro_batches

        # -------------------------------------------------------------------------
        # Run optimizer
        # -------------------------------------------------------------------------

        # calculate gradients, and log grad_norm
        if self.amp:
            self.scaler.scale(loss).backward()
        else:
            loss.backward()

    @profiler.record_function("grad_clip")
    def _log_and_clip_grad_norm(self, model, log_var_name="opt_grad"):
        if self.max_grad_norm is not None and self.max_grad_norm != 0:
            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)
        else:
            # even if we don't clip the gradient we should at least log the norm. This is probably a bit slow though.
            # we could do this every 10th step, but it's important that a large grad_norm doesn't get missed.
            grad_norm = 0
            parameters = list(filter(lambda p: p.grad is not None, model.parameters()))
            for p in parameters:
                param_norm = p.grad.data.norm(2)
                grad_norm += param_norm.item() ** 2
            grad_norm = grad_norm ** 0.5
        self.log.watch_mean(log_var_name, grad_norm)

    @profiler.record_function("minibatch_back")
    def opt_step_minibatch(self, model, optimizer, log_var_name):
        """ Runs the optimization step for a minimatch, forward_minibatch should be called first."""

        if self.amp:
            self.scaler.unscale_(optimizer)
            self._log_and_clip_grad_norm(model, log_var_name)
            self.scaler.step(optimizer)
            self.scaler.update()
            optimizer.zero_grad()
        else:
            self._log_and_clip_grad_norm(model, log_var_name)
            optimizer.step()
            optimizer.zero_grad()

    def _train_model(self, batch_data, model, optimizer, rnn_states, forward_func, short_name="model"):
        """
        :param batch_data: a dictionary containing data required for training
        :return:
        """
        for i in range(self.batch_epochs):

            assert self.mini_batch_size % self.n_steps == 0, "mini batch size must be multiple of n_steps."
            segments = list(range(self.n_agents))
            np.random.shuffle(segments)

            # figure out how many agents to run in parallel so that mini batch size is right
            # this will be num_segments * rnn_block_length
            assert self.mini_batch_size % self.micro_batches == 0
            segments_per_micro_batch = self.batch_size // self.n_steps // \
                                       (self.mini_batches * self.micro_batches)

            optimizer.zero_grad()

            micro_batch_counter = 0

            for j in range(self.mini_batches):
                with autocast(enabled=self.amp):
                    for k in range(self.micro_batches):
                        batch_start = micro_batch_counter * segments_per_micro_batch
                        batch_end = (micro_batch_counter + 1) * segments_per_micro_batch
                        sample = segments[batch_start:batch_end]

                        # initialize states from state taken during rollout, then upload to gpu
                        sampled_rnn_states = torch.from_numpy(rnn_states[sample]).clone().detach().to(device=model.device)

                        # put together a mini batch from all agents at this time step...
                        mini_batch_data = {}
                        for key, value in batch_data.items():
                            mini_batch_data[key] = torch.from_numpy(value[:, sample]).to(device=model.device)

                        forward_func(mini_batch_data, sampled_rnn_states)
                        micro_batch_counter += 1
                self.opt_step_minibatch(model, optimizer, short_name+"_grad")

    @profiler.record_function("train_deception")
    def train_deception(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        batch_data = {}
        batch_data["prev_obs"] = self.batch_prev_obs
        batch_data["player_obs"] = self.batch_player_obs
        batch_data["terminals"] = self.batch_terminals
        batch_data["roles"] = self.batch_roles

        self._train_model(
            batch_data,
            self.deception_model,
            self.deception_optimizer,
            self.prev_rnn_states[:, 2:4],
            self.forward_deception_mini_batch,
            "dec"
        )

    @profiler.record_function("train_policy")
    def train_policy(self):
        """ trains agent on it's own experience, using the rnn training loop """

        # put the required data into a dictionary
        batch_data = {}
        batch_data["prev_obs"] = self.batch_prev_obs
        batch_data["actions"] = self.batch_actions.astype(np.long)
        batch_data["ext_returns"] = self.batch_ext_returns
        batch_data["int_returns"] = self.batch_int_returns
        batch_data["ext_value"] = self.batch_ext_value
        batch_data["int_value"] = self.batch_int_value
        batch_data["log_policy"] = self.batch_log_policy
        batch_data["advantages"] = self.batch_advantage
        batch_data["terminals"] = self.batch_terminals

        self._train_model(
            batch_data,
            self.policy_model,
            self.policy_optimizer,
            self.prev_rnn_states[:, 0:2],
            self.forward_policy_mini_batch,
            "pol"
        )

# ------------------------------------------------------------------
# Helper functions
# ------------------------------------------------------------------

def calculate_returns(rewards, dones, final_value_estimate, gamma):
    """
    Calculates returns given a batch of rewards, dones, and a final value estimate.
    Input is vectorized so it can calculate returns for multiple agents at once.
    :param rewards: nd array of dims [N,A]
    :param dones:   nd array of dims [N,A] where 1 = done and 0 = not done for given timestep
    :param final_value_estimate: nd array [A] containing value estimate of final state after last action.
    :param gamma:   discount rate.
    :return:
    """

    N, A = rewards.shape

    returns = np.zeros([N, A], dtype=np.float32)
    current_return = final_value_estimate

    for i in reversed(range(N)):
        returns[i] = current_return = rewards[i] + current_return * gamma * (1.0 - dones[i])

    return returns

def calculate_gae(batch_rewards, batch_value, final_value_estimate, batch_terminal, final_terminal, gamma, lamb):

    N, A = batch_rewards.shape

    batch_advantage = np.zeros_like(batch_rewards, dtype=np.float32)
    prev_adv = np.zeros([A], dtype=np.float32)
    for t in reversed(range(N)):

        if t != N-1:
            is_next_terminal = batch_terminal[t + 1]
            value_next_t = batch_value[t + 1]
        else:
            is_next_terminal = final_terminal
            value_next_t = final_value_estimate

        delta = batch_rewards[t] + gamma * value_next_t * (1.0 - is_next_terminal) - batch_value[t]
        batch_advantage[t] = prev_adv = delta + gamma * lamb * (
                1.0 - is_next_terminal) * prev_adv

    return batch_advantage

def merge_down(x):
    """ Combines first two dims. """
    return x.reshape(-1, *x.shape[2:])

def test_calculate_returns():
    rewards = np.asarray([[0], [2], [1], [4], [1]])
    dones = np.asarray([[0], [0], [0], [1], [0]])
    assert list(calculate_returns(rewards, dones, final_value_estimate=6, gamma=1.0).ravel()) == [7, 7, 5, 4, 7]


test_calculate_returns()